dtrizna@tanuki:~/QuasarNix/experiments$ python3 train_full_data.py
Python implementation: CPython
Python version       : 3.10.12
IPython version      : 8.22.2

torch    : 2.2.1
lightning: 2.2.1
sklearn  : 1.4.1.post1

[!] Script start time: Fri Aug 16 18:50:05 2024
Seed set to 33
[!] X_train_malicious_cmd: 266501 | X_test_malicious_cmd: 235060
[!] X_train_baseline_cmd: 266513 | X_test_baseline_cmd: 235069
[*] Loading adversarial training set from:
        'logs_adv_train_full/X_train_malicious_cmd_adv.json'
[*] Loading adversarial test set from:
        'logs_adv_train_full/X_test_malicious_cmd_adv.json'
[*] Loading vocab from:
        'logs_adv_train_full/wordpunct_vocab_4096.json'
[*] Creating dataloaders from commands...
[*] Loading One-Hot encoder from:
        'logs_adv_train_full/onehot_vectorizer_4096.pkl'
/home/dtrizna/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
[!] Training of _tabular_mlp_onehot started:  Fri Aug 16 18:52:06 2024
Seed set to 42
[!] Logging to logs_adv_train_full
/home/dtrizna/.local/lib/python3.10/site-packages/lightning/fabric/connector.py:563: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
/home/dtrizna/.local/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 train_full_data.py ...
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[!] Scheduler: onecycle | Scheduler step budget: 9800
[*] Training '_tabular_mlp_onehot' model...
/home/dtrizna/.local/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 train_full_data.py ...
You are using a CUDA device ('NVIDIA L40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[!] Setting up onecycle scheduler with step budget 9800
Epoch 0: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [01:08<00:00, 14.26it/s, v_num=0, train_loss=0.0105, memory=3.81e+7, val_f1=1.000, val_tpr=0.999, val_acc=1.000, train_f1=0.938, train_tpr=0.826, train_acc=0.934]
Epoch 1: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [01:02<00:00, 15.79it/s, v_num=0, train_loss=0.000503, memory=3.81e+7, val_f1=1.000, val_tpr=1.000, val_acc=1.000, train_f1=1.000, train_tpr=0.998, train_acc=1.000]
Epoch 2: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [01:03<00:00, 15.43it/s, v_num=0, train_loss=0.000184, memory=3.81e+7, val_f1=1.000, val_tpr=1.000, val_acc=1.000, train_f1=1.000, train_tpr=0.998, train_acc=1.000]
Epoch 3: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [01:02<00:00, 15.64it/s, v_num=0, train_loss=3e-5, memory=3.81e+7, val_f1=1.000, val_tpr=1.000, val_acc=1.000, train_f1=1.000, train_tpr=0.999, train_acc=1.000]
Epoch 4: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [01:00<00:00, 16.25it/s, v_num=0, train_loss=4.69e-5, memory=3.81e+7, val_f1=1.000, val_tpr=1.000, val_acc=1.000, train_f1=1.000, train_tpr=0.999, train_acc=1.000]
Epoch 5: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [01:08<00:00, 14.29it/s, v_num=0, train_loss=1.14e-5, memory=3.81e+7, val_f1=1.000, val_tpr=1.000, val_acc=1.000, train_f1=1.000, train_tpr=0.999, train_acc=1.000]
Epoch 6: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [01:07<00:00, 14.46it/s, v_num=0, train_loss=0.000451, memory=3.81e+7, val_f1=1.000, val_tpr=1.000, val_acc=1.000, train_f1=1.000, train_tpr=0.999, train_acc=1.000]
Epoch 7: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [01:00<00:00, 16.18it/s, v_num=0, train_loss=3.19e-5, memory=3.81e+7, val_f1=1.000, val_tpr=1.000, val_acc=1.000, train_f1=1.000, train_tpr=0.998, train_acc=1.000]
Epoch 8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [01:02<00:00, 15.73it/s, v_num=0, train_loss=1.91e-5, memory=3.81e+7, val_f1=1.000, val_tpr=1.000, val_acc=1.000, train_f1=1.000, train_tpr=0.999, train_acc=1.000]
Epoch 9: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [00:59<00:00, 16.44it/s, v_num=0, train_loss=2.42e-5, memory=3.81e+7, val_f1=1.000, val_tpr=1.000, val_acc=1.000, train_f1=1.000, train_tpr=0.999, train_acc=1.000]
`Trainer.fit` stopped: `max_epochs=10` reached.
Epoch 9: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [00:59<00:00, 16.44it/s, v_num=0, train_loss=2.42e-5, memory=3.81e+7, val_f1=1.000, val_tpr=1.000, val_acc=1.000, train_f1=1.000, train_tpr=0.999, train_acc=1.000]
[!] Training of _tabular_mlp_onehot ended:  Fri Aug 16 19:02:45 2024  | Took: 639.21 seconds
[!] Training of _tabular_xgb_onehot started:  Fri Aug 16 19:02:45 2024
[*] Training _tabular_xgb_onehot model...
[!] _tabular_xgb_onehot model scores: train_tpr=1.0000, train_f1=1.0000, train_acc=1.0000, train_auc=1.0000
[!] _tabular_xgb_onehot model scores: val_tpr=1.0000, val_f1=1.0000, val_acc=1.0000, val_auc=1.0000
[!] Training of _tabular_xgb_onehot ended:  Fri Aug 16 19:03:30 2024  | Took: 44.94 seconds
[!] Training of cnn started:  Fri Aug 16 19:03:30 2024
Seed set to 42
[!] Logging to logs_adv_train_full
/home/dtrizna/.local/lib/python3.10/site-packages/lightning/fabric/connector.py:563: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
/home/dtrizna/.local/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 train_full_data.py ...
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[!] Scheduler: onecycle | Scheduler step budget: 9800
[*] Training 'cnn' model...
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[!] Setting up onecycle scheduler with step budget 9800
Epoch 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [00:28<00:00, 34.90it/s, v_num=0, train_loss=0.00108, memory=9.09e+7, val_f1=1.000, val_tpr=0.996, val_acc=1.000, train_f1=0.946, train_tpr=0.591, train_acc=0.946]
Epoch 1: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [00:30<00:00, 32.28it/s, v_num=0, train_loss=0.000116, memory=9.09e+7, val_f1=1.000, val_tpr=1.000, val_acc=1.000, train_f1=1.000, train_tpr=0.995, train_acc=1.000]
Epoch 2: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [00:33<00:00, 29.45it/s, v_num=0, train_loss=3.11e-5, memory=9.09e+7, val_f1=1.000, val_tpr=1.000, val_acc=1.000, train_f1=1.000, train_tpr=0.999, train_acc=1.000]
Epoch 3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [00:31<00:00, 31.00it/s, v_num=0, train_loss=3.24e-5, memory=9.09e+7, val_f1=1.000, val_tpr=1.000, val_acc=1.000, train_f1=1.000, train_tpr=1.000, train_acc=1.000]
Epoch 4: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [00:29<00:00, 33.17it/s, v_num=0, train_loss=4.01e-6, memory=9.09e+7, val_f1=1.000, val_tpr=1.000, val_acc=1.000, train_f1=1.000, train_tpr=1.000, train_acc=1.000]
Epoch 5: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [00:35<00:00, 27.89it/s, v_num=0, train_loss=3.32e-5, memory=9.09e+7, val_f1=1.000, val_tpr=1.000, val_acc=1.000, train_f1=1.000, train_tpr=1.000, train_acc=1.000]
Epoch 6: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [00:31<00:00, 31.09it/s, v_num=0, train_loss=2.92e-6, memory=9.09e+7, val_f1=1.000, val_tpr=1.000, val_acc=1.000, train_f1=1.000, train_tpr=1.000, train_acc=1.000]
Epoch 7: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [00:31<00:00, 30.79it/s, v_num=0, train_loss=2.28e-7, memory=9.09e+7, val_f1=1.000, val_tpr=1.000, val_acc=1.000, train_f1=1.000, train_tpr=1.000, train_acc=1.000]
Epoch 8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [00:30<00:00, 31.91it/s, v_num=0, train_loss=1.76e-6, memory=9.09e+7, val_f1=1.000, val_tpr=1.000, val_acc=1.000, train_f1=1.000, train_tpr=1.000, train_acc=1.000]
Epoch 9: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [00:34<00:00, 28.62it/s, v_num=0, train_loss=1.04e-7, memory=9.09e+7, val_f1=1.000, val_tpr=1.000, val_acc=1.000, train_f1=1.000, train_tpr=1.000, train_acc=1.000]
`Trainer.fit` stopped: `max_epochs=10` reached.
Epoch 9: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [00:34<00:00, 28.62it/s, v_num=0, train_loss=1.04e-7, memory=9.09e+7, val_f1=1.000, val_tpr=1.000, val_acc=1.000, train_f1=1.000, train_tpr=1.000, train_acc=1.000]
[!] Training of cnn ended:  Fri Aug 16 19:08:47 2024  | Took: 317.21 seconds
[!] Training of cls_transformer started:  Fri Aug 16 19:08:47 2024
Seed set to 42
[!] Logging to logs_adv_train_full
/home/dtrizna/.local/lib/python3.10/site-packages/lightning/fabric/connector.py:563: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
/home/dtrizna/.local/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 train_full_data.py ...
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[!] Scheduler: onecycle | Scheduler step budget: 9800
[*] Training 'cls_transformer' model...
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[!] Setting up onecycle scheduler with step budget 9800
Epoch 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [00:44<00:00, 21.78it/s, v_num=0, train_loss=0.045, memory=3.77e+8, val_f1=0.994, val_tpr=0.331, val_acc=0.994, train_f1=0.851, train_tpr=0.156, train_acc=0.857]
Epoch 1: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [00:45<00:00, 21.43it/s, v_num=0, train_loss=0.00545, memory=3.77e+8, val_f1=0.999, val_tpr=0.791, val_acc=0.999, train_f1=0.996, train_tpr=0.772, train_acc=0.996]
Epoch 2: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [00:45<00:00, 21.54it/s, v_num=0, train_loss=0.00027, memory=3.77e+8, val_f1=1.000, val_tpr=0.895, val_acc=1.000, train_f1=0.999, train_tpr=0.943, train_acc=0.999]
Epoch 3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [00:46<00:00, 21.26it/s, v_num=0, train_loss=9.55e-5, memory=3.77e+8, val_f1=1.000, val_tpr=0.900, val_acc=1.000, train_f1=1.000, train_tpr=0.976, train_acc=1.000]
Epoch 4: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [00:43<00:00, 22.58it/s, v_num=0, train_loss=2.98e-5, memory=3.77e+8, val_f1=1.000, val_tpr=0.933, val_acc=1.000, train_f1=1.000, train_tpr=0.987, train_acc=1.000]
Epoch 5: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [00:44<00:00, 21.87it/s, v_num=0, train_loss=1.92e-5, memory=3.77e+8, val_f1=1.000, val_tpr=0.979, val_acc=1.000, train_f1=1.000, train_tpr=0.990, train_acc=1.000]
Epoch 6: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [00:44<00:00, 21.86it/s, v_num=0, train_loss=1.09e-5, memory=3.77e+8, val_f1=1.000, val_tpr=0.962, val_acc=1.000, train_f1=1.000, train_tpr=0.995, train_acc=1.000]
Epoch 7: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [00:45<00:00, 21.40it/s, v_num=0, train_loss=1e-5, memory=3.77e+8, val_f1=1.000, val_tpr=0.974, val_acc=1.000, train_f1=1.000, train_tpr=0.997, train_acc=1.000]
Epoch 8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [00:45<00:00, 21.33it/s, v_num=0, train_loss=8.72e-7, memory=3.77e+8, val_f1=1.000, val_tpr=0.979, val_acc=1.000, train_f1=1.000, train_tpr=0.997, train_acc=1.000]
Epoch 9: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [00:43<00:00, 22.28it/s, v_num=0, train_loss=1.17e-6, memory=3.77e+8, val_f1=1.000, val_tpr=0.979, val_acc=1.000, train_f1=1.000, train_tpr=0.997, train_acc=1.000]
`Trainer.fit` stopped: `max_epochs=10` reached.
Epoch 9: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 980/980 [00:44<00:00, 22.27it/s, v_num=0, train_loss=1.17e-6, memory=3.77e+8, val_f1=1.000, val_tpr=0.979, val_acc=1.000, train_f1=1.000, train_tpr=0.997, train_acc=1.000]
[!] Training of cls_transformer ended:  Fri Aug 16 19:16:20 2024  | Took: 452.62 seconds
[!] Script end time: Fri Aug 16 19:16:20 2024