{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes of train and test sets: 533014, 470128\n",
      "Python implementation: CPython\n",
      "Python version       : 3.10.12\n",
      "IPython version      : 8.22.2\n",
      "\n",
      "torch    : 2.2.1\n",
      "lightning: 2.2.1\n",
      "sklearn  : 1.4.1.post1\n",
      "\n",
      "[!] Script start time: Tue Jun 24 10:02:27 2025\n",
      "[*] Loading vocab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing commands: 100%|██████████| 470128/470128 [00:04<00:00, 116663.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Loading MinHash vectorizer...\n",
      "[*] Loading One-Hot vectorizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing sequences: 100%|██████████| 470128/470128 [00:03<00:00, 147887.85it/s]\n",
      "Encoding sequences: 100%|██████████| 470128/470128 [00:20<00:00, 22469.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Defining models...\n",
      "Best model:  logs_models_ACM_TOPS_v3/_tabular_mlp_onehot_csv/version_0/checkpoints/epoch=0-tprval_tpr=0.9910-f1val_f1=0.9982-accval_cc=0.0000.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dtrizna/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/home/dtrizna/.local/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 /home/dtrizna/.local/lib/python3.10/site-packages/i ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Predicting with _tabular_mlp_onehot...\n",
      "Predicting DataLoader 0: 100%|██████████| 460/460 [00:13<00:00, 35.38it/s]\n",
      "[*] Loading _tabular_rf_onehot from logs_models_ACM_TOPS_v3/_tabular_rf_onehot/model.pkl...\n",
      "[*] Predicting with _tabular_rf_onehot...\n",
      "[*] Loading _tabular_xgb_onehot from logs_models_ACM_TOPS_v3/_tabular_xgb_onehot/model.xgboost...\n",
      "[*] Predicting with _tabular_xgb_onehot...\n",
      "Best model:  logs_models_ACM_TOPS_v3/mlp_seq_csv/version_0/checkpoints/epoch=7-tprval_tpr=0.6473-f1val_f1=0.7073-accval_cc=0.0000.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Predicting with mlp_seq...\n",
      "Predicting DataLoader 0: 100%|██████████| 460/460 [00:02<00:00, 227.25it/s]\n",
      "Best model:  logs_models_ACM_TOPS_v3/attpool_transformer_csv/version_0/checkpoints/epoch=6-tprval_tpr=0.8185-f1val_f1=0.9389-accval_cc=0.0000.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Predicting with attpool_transformer...\n",
      "Predicting DataLoader 0: 100%|██████████| 460/460 [00:06<00:00, 73.53it/s]\n",
      "Best model:  logs_models_ACM_TOPS_v3/cls_transformer_csv/version_0/checkpoints/epoch=3-tprval_tpr=0.9299-f1val_f1=0.9983-accval_cc=0.0000.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Predicting with cls_transformer...\n",
      "Predicting DataLoader 0: 100%|██████████| 460/460 [00:07<00:00, 59.22it/s]\n",
      "Best model:  logs_models_ACM_TOPS_v3/mean_transformer_csv/version_0/checkpoints/epoch=5-tprval_tpr=0.7533-f1val_f1=0.8855-accval_cc=0.0000.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Predicting with mean_transformer...\n",
      "Predicting DataLoader 0: 100%|██████████| 460/460 [00:06<00:00, 75.95it/s]\n",
      "Best model:  logs_models_ACM_TOPS_v3/neurlux_csv/version_0/checkpoints/epoch=0-tprval_tpr=0.6855-f1val_f1=0.7158-accval_cc=0.0000.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Predicting with neurlux...\n",
      "Predicting DataLoader 0: 100%|██████████| 460/460 [00:01<00:00, 234.13it/s]\n",
      "Best model:  logs_models_ACM_TOPS_v3/cnn_csv/version_0/checkpoints/epoch=12-tprval_tpr=0.9998-f1val_f1=0.9375-accval_cc=0.0000.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Predicting with cnn...\n",
      "Predicting DataLoader 0: 100%|██████████| 460/460 [00:01<00:00, 300.70it/s]\n",
      "Best model:  logs_models_ACM_TOPS_v3/lstm_csv/version_0/checkpoints/epoch=19-tprval_tpr=0.7474-f1val_f1=0.5333-accval_cc=0.0000.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Predicting with lstm...\n",
      "Predicting DataLoader 0: 100%|██████████| 460/460 [00:01<00:00, 298.80it/s]\n",
      "Best model:  logs_models_ACM_TOPS_v3/cnn_lstm_csv/version_0/checkpoints/epoch=4-tprval_tpr=0.8290-f1val_f1=0.7255-accval_cc=0.0000.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Predicting with cnn_lstm...\n",
      "Predicting DataLoader 0: 100%|██████████| 460/460 [00:01<00:00, 291.43it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "import sys\n",
    "ROOT = \"..\"\n",
    "sys.path.append(ROOT)\n",
    "\n",
    "from src.data_utils import load_data\n",
    "\n",
    "SEED = 33\n",
    "VOCAB_SIZE = 4096\n",
    "EMBEDDED_DIM = 64\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 1024\n",
    "DROPOUT = 0.5\n",
    "LIMIT = None\n",
    "DATALOADER_WORKERS = 4\n",
    "\n",
    "\n",
    "# ===========================================\n",
    "# LOADING DATA\n",
    "# ===========================================\n",
    "\n",
    "X_train_cmds, y_train, X_test_cmds, y_test, *_ = load_data(seed=SEED, limit=LIMIT, baseline=\"real\", root=ROOT)\n",
    "print(f\"Sizes of train and test sets: {len(X_train_cmds)}, {len(X_test_cmds)}\")\n",
    "\n",
    "LOGS_FOLDER = \"logs_models_ACM_TOPS_v3\"\n",
    "y_preds_pickle = os.path.join(LOGS_FOLDER, \"y_preds.pkl\")\n",
    "if os.path.exists(y_preds_pickle):\n",
    "    print(\"[*] Loading predictions...\")\n",
    "    with open(y_preds_pickle, \"rb\") as f:\n",
    "        y_preds = pickle.load(f)\n",
    "else:\n",
    "    y_preds = {}\n",
    "\n",
    "    from watermark import watermark\n",
    "\n",
    "    print(watermark(packages=\"torch,lightning,sklearn\", python=True))\n",
    "    print(f\"[!] Script start time: {time.ctime()}\")\n",
    "\n",
    "    # encoders\n",
    "    from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "    # tokenizers\n",
    "    from nltk.tokenize import wordpunct_tokenize, WhitespaceTokenizer\n",
    "    whitespace_tokenize = WhitespaceTokenizer().tokenize\n",
    "\n",
    "    # modeling\n",
    "    import lightning as L\n",
    "    from lightning.pytorch.loggers import CSVLogger, TensorBoardLogger\n",
    "    from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "    from lightning.fabric.utilities.seed import seed_everything\n",
    "\n",
    "    # import random forest, xgboost, and logistic regression\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "    from src.models import *\n",
    "    from src.lit_utils import LitProgressBar\n",
    "    from src.preprocessors import CommandTokenizer, OneHotCustomVectorizer\n",
    "    from src.data_utils import create_dataloader\n",
    "\n",
    "    from typing import List\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def get_tpr_at_fpr(predicted_logits, true_labels, fprNeeded=1e-4):\n",
    "        if isinstance(predicted_logits, torch.Tensor):\n",
    "            predicted_probs = torch.sigmoid(predicted_logits).cpu().detach().numpy()\n",
    "        else:\n",
    "            predicted_probs = sigmoid(predicted_logits)\n",
    "        \n",
    "        if isinstance(true_labels, torch.Tensor):\n",
    "            true_labels = true_labels.cpu().detach().numpy()\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(true_labels, predicted_probs)\n",
    "        if all(np.isnan(fpr)):\n",
    "            return np.nan#, np.nan\n",
    "        else:\n",
    "            tpr_at_fpr = tpr[fpr <= fprNeeded][-1]\n",
    "            #threshold_at_fpr = thresholds[fpr <= fprNeeded][-1]\n",
    "            return tpr_at_fpr#, threshold_at_fpr\n",
    "\n",
    "\n",
    "    def commands_to_loader(cmd: List[str], tokenizer: CommandTokenizer, y: np.ndarray = None) -> DataLoader:\n",
    "        \"\"\"Convert a list of commands to a DataLoader.\"\"\"\n",
    "        tokens = tokenizer.tokenize(cmd)\n",
    "        ints = tokenizer.encode(tokens)\n",
    "        padded = tokenizer.pad(ints)\n",
    "        if y is None:\n",
    "            loader = create_dataloader(padded, batch_size=BATCH_SIZE, workers=DATALOADER_WORKERS)\n",
    "        else:\n",
    "            loader = create_dataloader(padded, y, batch_size=BATCH_SIZE, workers=DATALOADER_WORKERS)\n",
    "        return loader\n",
    "\n",
    "\n",
    "    def configure_trainer():\n",
    "        \"\"\"Configure the PyTorch Lightning Trainer.\"\"\"\n",
    "\n",
    "        trainer = L.Trainer(\n",
    "            num_sanity_val_steps=0,\n",
    "            max_epochs=1,\n",
    "            accelerator=\"gpu\",\n",
    "            devices=1,\n",
    "            callbacks=[LitProgressBar()],\n",
    "            logger=TensorBoardLogger(\"logs_temp_results_roc_ablation_models\", name=\"my_model\"),\n",
    "            val_check_interval=0.5,\n",
    "            log_every_n_steps=10,\n",
    "        )\n",
    "        return trainer\n",
    "\n",
    "\n",
    "    def load_lit_model(model_file, pytorch_model, name, log_folder):\n",
    "        lightning_model = PyTorchLightningModel.load_from_checkpoint(\n",
    "            checkpoint_path=model_file,\n",
    "            model=pytorch_model,\n",
    "            learning_rate=1e-3,\n",
    "        )\n",
    "        trainer = configure_trainer()\n",
    "        return trainer, lightning_model\n",
    "\n",
    "    seed_everything(SEED)\n",
    "    TOKENIZER = wordpunct_tokenize\n",
    "\n",
    "    # =============================================\n",
    "    # PREPING DATA\n",
    "    # =============================================\n",
    "    tokenizer = CommandTokenizer(tokenizer_fn=TOKENIZER, vocab_size=VOCAB_SIZE, max_len=MAX_LEN)\n",
    "\n",
    "    # ========== EMBEDDING ==========\n",
    "    vocab_file = os.path.join(LOGS_FOLDER, f\"wordpunct_vocab_{VOCAB_SIZE}.json\")\n",
    "    if os.path.exists(vocab_file):\n",
    "        print(\"[*] Loading vocab...\")\n",
    "        tokenizer.load_vocab(vocab_file)\n",
    "    else:\n",
    "        print(\"[*] Building vocab and encoding...\")\n",
    "        X_train_tokens = tokenizer.tokenize(X_train_cmds)\n",
    "        tokenizer.build_vocab(X_train_tokens)\n",
    "        tokenizer.dump_vocab(vocab_file)\n",
    "\n",
    "    # creating dataloaders\n",
    "    # X_train_loader = commands_to_loader(X_train_cmds, tokenizer, y_train)\n",
    "    X_test_loader = commands_to_loader(X_test_cmds, tokenizer, y_test)\n",
    "\n",
    "    # ========== MIN-HASH TABULAR ENCODING ==========\n",
    "    minhash_vectorizer_file = os.path.join(LOGS_FOLDER, f\"minhash_vectorizer_{VOCAB_SIZE}.pkl\")\n",
    "    if os.path.exists(minhash_vectorizer_file):\n",
    "        print(\"[*] Loading MinHash vectorizer...\")\n",
    "        minhash = pickle.load(open(minhash_vectorizer_file, \"rb\"))\n",
    "    else:\n",
    "        minhash = HashingVectorizer(n_features=VOCAB_SIZE, tokenizer=TOKENIZER, token_pattern=None)\n",
    "        print(\"[*] Fitting MinHash encoder...\")\n",
    "        minhash.fit(X_train_cmds)\n",
    "        \n",
    "        with open(minhash_vectorizer_file, \"wb\") as f:\n",
    "            pickle.dump(minhash, f)\n",
    "\n",
    "    # X_train_minhash = minhash.transform(X_train_cmds)\n",
    "    X_test_minhash = minhash.transform(X_test_cmds)\n",
    "\n",
    "    # ========== ONE-HOT TABULAR ENCODING ===========\n",
    "    oh_vectorizer_file = os.path.join(LOGS_FOLDER, f\"onehot_vectorizer_{VOCAB_SIZE}.pkl\")\n",
    "    if os.path.exists(oh_vectorizer_file):\n",
    "        print(\"[*] Loading One-Hot vectorizer...\")\n",
    "        oh = pickle.load(open(oh_vectorizer_file, \"rb\"))\n",
    "    else:\n",
    "        oh = OneHotCustomVectorizer(tokenizer=TOKENIZER, max_features=VOCAB_SIZE)\n",
    "        print(\"[*] Fitting One-Hot encoder...\")\n",
    "        oh.fit(X_train_cmds)\n",
    "\n",
    "        with open(oh_vectorizer_file, \"wb\") as f:\n",
    "            pickle.dump(oh, f)\n",
    "\n",
    "    # X_train_onehot = oh.transform(X_train_cmds)\n",
    "    X_test_onehot = oh.transform(X_test_cmds)\n",
    "\n",
    "    # =============================================\n",
    "    # DEFINING MODELS\n",
    "    # =============================================\n",
    "    print(f\"[*] Defining models...\")\n",
    "\n",
    "    # sequence models\n",
    "    mlp_seq_model = SimpleMLPWithEmbedding(vocab_size=VOCAB_SIZE, embedding_dim=EMBEDDED_DIM, output_dim=1, hidden_dim=[256, 64, 32], use_positional_encoding=False, max_len=MAX_LEN, dropout=DROPOUT) # 297 K params\n",
    "    cnn_model = CNN1DGroupedModel(vocab_size=VOCAB_SIZE, embed_dim=EMBEDDED_DIM, num_channels=32, kernel_sizes=[2, 3, 4, 5], mlp_hidden_dims=[64, 32], output_dim=1, dropout=DROPOUT) # 301 K params\n",
    "    lstm_model = BiLSTMModel(vocab_size=VOCAB_SIZE, embed_dim=EMBEDDED_DIM, hidden_dim=32, mlp_hidden_dims=[64, 32], output_dim=1, dropout=DROPOUT) # 318 K params\n",
    "    cnn_lstm_model = CNN1D_BiLSTM_Model(vocab_size=VOCAB_SIZE, embed_dim=EMBEDDED_DIM, num_channels=32, kernel_size=3, lstm_hidden_dim=32, mlp_hidden_dims=[64, 32], output_dim=1, dropout=DROPOUT) # 316 K params\n",
    "    mean_transformer_model = MeanTransformerEncoder(vocab_size=VOCAB_SIZE, d_model=EMBEDDED_DIM, nhead=4, num_layers=2, dim_feedforward=128, max_len=MAX_LEN, dropout=DROPOUT, mlp_hidden_dims=[64,32], output_dim=1) # 335 K params\n",
    "    cls_transformer_model = CLSTransformerEncoder(vocab_size=VOCAB_SIZE, d_model=EMBEDDED_DIM, nhead=4, num_layers=2, dim_feedforward=128, max_len=MAX_LEN, dropout=DROPOUT, mlp_hidden_dims=[64,32], output_dim=1) #  335 K params\n",
    "    attpool_transformer_model = AttentionPoolingTransformerEncoder(vocab_size=VOCAB_SIZE, d_model=EMBEDDED_DIM, nhead=4, num_layers=2, dim_feedforward=128, max_len=MAX_LEN, dropout=DROPOUT, mlp_hidden_dims=[64,32], output_dim=1) #  335 K params\n",
    "    neurlux = NeurLuxModel(vocab_size=VOCAB_SIZE, embed_dim=EMBEDDED_DIM, max_len=MAX_LEN, hidden_dim=32, output_dim=1, dropout=DROPOUT) # 402 K params\n",
    "\n",
    "    # tabular models\n",
    "    rf_model_minhash = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=SEED)\n",
    "    xgb_model_minhash = XGBClassifier(n_estimators=100, max_depth=10, random_state=SEED)\n",
    "    log_reg_minhash = LogisticRegression(random_state=SEED)\n",
    "    mlp_tab_model_minhash = SimpleMLP(input_dim=VOCAB_SIZE, output_dim=1, hidden_dim=[64, 32], dropout=DROPOUT) # 264 K params\n",
    "    rf_model_onehot = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=SEED)\n",
    "    xgb_model_onehot = XGBClassifier(n_estimators=100, max_depth=10, random_state=SEED)\n",
    "    log_reg_onehot = LogisticRegression(random_state=SEED)\n",
    "    mlp_tab_model_onehot = SimpleMLP(input_dim=VOCAB_SIZE, output_dim=1, hidden_dim=[64, 32], dropout=DROPOUT) # 264 K params\n",
    "\n",
    "    models = {\n",
    "        # \"_tabular_mlp_minhash\": mlp_tab_model_minhash,\n",
    "        # \"_tabular_rf_minhash\": rf_model_minhash,\n",
    "        # \"_tabular_xgb_minhash\": xgb_model_minhash,\n",
    "        # \"_tabular_log_reg_minhash\": log_reg_minhash,\n",
    "        \"_tabular_mlp_onehot\": mlp_tab_model_onehot,\n",
    "        \"_tabular_rf_onehot\": rf_model_onehot,\n",
    "        \"_tabular_xgb_onehot\": xgb_model_onehot,\n",
    "        # \"_tabular_log_reg_onehot\": log_reg_onehot,\n",
    "        \"mlp_seq\": mlp_seq_model,\n",
    "        \"attpool_transformer\": attpool_transformer_model,\n",
    "        \"cls_transformer\": cls_transformer_model,\n",
    "        \"mean_transformer\": mean_transformer_model,\n",
    "        \"neurlux\": neurlux,\n",
    "        \"cnn\": cnn_model,\n",
    "        \"lstm\": lstm_model,\n",
    "        \"cnn_lstm\": cnn_lstm_model,\n",
    "    }\n",
    "\n",
    "    for name, model in models.items():\n",
    "        if name in y_preds:\n",
    "            print(f\"[*] Model {name} already predicted\")\n",
    "            continue\n",
    "\n",
    "        if name.startswith(\"_tabular\") and \"mlp\" not in name:\n",
    "            if \"xgb\" in name:\n",
    "                model_file = os.path.join(LOGS_FOLDER, name, \"model.xgboost\")\n",
    "            else:\n",
    "                model_file = os.path.join(LOGS_FOLDER, name, \"model.pkl\")\n",
    "            print(f\"[*] Loading {name} from {model_file}...\")\n",
    "            if \"xgb\" in name:\n",
    "                model = XGBClassifier()\n",
    "                model.load_model(model_file)\n",
    "            else:\n",
    "                with open(model_file, \"rb\") as f:\n",
    "                    model = pickle.load(f)\n",
    "            \n",
    "            preprocessor = name.split(\"_\")[-1]\n",
    "            assert preprocessor in [\"onehot\", \"minhash\"]\n",
    "\n",
    "            if preprocessor == \"onehot\":\n",
    "                # x_train = X_train_onehot\n",
    "                x_test = X_test_onehot\n",
    "            \n",
    "            elif preprocessor == \"minhash\":\n",
    "                # x_train = X_train_minhash\n",
    "                x_test = X_test_minhash\n",
    "\n",
    "            print(f\"[*] Predicting with {name}...\")\n",
    "            y_test_preds = model.predict_proba(x_test)[:,1]\n",
    "            y_preds[name] = y_test_preds\n",
    "        \n",
    "        else:    \n",
    "            if \"tabular\" in name:\n",
    "                preprocessor = name.split(\"_\")[-1]\n",
    "                assert preprocessor in [\"onehot\", \"minhash\"]\n",
    "\n",
    "                if preprocessor == \"onehot\":\n",
    "                    # x_train = X_train_onehot\n",
    "                    x_test = X_test_onehot\n",
    "                \n",
    "                elif preprocessor == \"minhash\":\n",
    "                    # x_train = X_train_minhash\n",
    "                    x_test = X_test_minhash\n",
    "\n",
    "                # train_loader = create_dataloader(x_train, y_train, batch_size=BATCH_SIZE, workers=DATALOADER_WORKERS)\n",
    "                test_loader = create_dataloader(x_test, y_test, batch_size=BATCH_SIZE, workers=DATALOADER_WORKERS)\n",
    "            \n",
    "            else:\n",
    "                # train_loader = X_train_loader\n",
    "                test_loader = X_test_loader\n",
    "            \n",
    "            chkp_folder = os.path.join(LOGS_FOLDER, f\"{name}_csv\", \"version_0\", \"checkpoints\")\n",
    "            if not os.path.exists(chkp_folder):\n",
    "                print(f\"Model {name} not trained yet\")\n",
    "\n",
    "            best_model = [x for x in os.listdir(os.path.join(LOGS_FOLDER, f\"{name}_csv\", \"version_0\", \"checkpoints\")) if x.startswith(\"epoch\")][0]\n",
    "            best_model = os.path.join(chkp_folder, best_model)\n",
    "            print(\"Best model: \", best_model)\n",
    "            trainer, lightning_model = load_lit_model(best_model, model, name, LOGS_FOLDER)\n",
    "            print(f\"[*] Predicting with {name}...\")\n",
    "            y_pred_proba = trainer.predict(lightning_model, test_loader, return_predictions=True)\n",
    "            if isinstance(y_pred_proba, list):\n",
    "                y_pred_proba = np.vstack(y_pred_proba).squeeze()\n",
    "\n",
    "            y_preds[name] = y_pred_proba\n",
    "\n",
    "    with open(y_preds_pickle, \"wb\") as f:\n",
    "        pickle.dump(y_preds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing commands:   2%|▏         | 9832/470128 [00:00<00:04, 98294.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing commands: 100%|██████████| 470128/470128 [00:04<00:00, 104863.62it/s]\n",
      "/home/dtrizna/.local/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 /home/dtrizna/.local/lib/python3.10/site-packages/i ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 460/460 [00:05<00:00, 82.98it/s] \n"
     ]
    }
   ],
   "source": [
    "best_model = \"logs_augm_no_augm/real_test_set_cnn_1750779606/not_augm_cnn_csv/version_0/checkpoints/epoch=9-val_tpr=0.4703-val_f1=0.0000-val_cc=0.0000.ckpt\"\n",
    "cnn_model_non_aug = CNN1DGroupedModel(vocab_size=VOCAB_SIZE, embed_dim=EMBEDDED_DIM, num_channels=32, kernel_sizes=[2, 3, 4, 5], mlp_hidden_dims=[64, 32], output_dim=1, dropout=DROPOUT) # 301 K params\n",
    "X_test_loader = commands_to_loader(X_test_cmds, tokenizer, y_test)\n",
    "test_loader = X_test_loader\n",
    "\n",
    "trainer, lightning_model = load_lit_model(best_model, cnn_model_non_aug, \"cnn_non_augm\", LOGS_FOLDER)\n",
    "y_pred_proba = trainer.predict(lightning_model, test_loader, return_predictions=True)\n",
    "if isinstance(y_pred_proba, list):\n",
    "                y_pred_proba = np.vstack(y_pred_proba).squeeze()\n",
    "y_preds[\"cnn_non_augm\"] = y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from src.plots import set_size, plot_roc_curve\n",
    "# import scienceplots\n",
    "# plt.style.use(['science', 'no-latex'])\n",
    "plt.rc('text', usetex=True)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=set_size())\n",
    "\n",
    "# make axins to show xlim 3e-6, 2e-5 and ylim 0.94, 1.02\n",
    "axin = ax.inset_axes([0.12, 0.45, 0.3, 0.33])\n",
    "\n",
    "to_skip = [x for x in y_preds.keys() if 'tabular' in x]\n",
    "to_keep = {\n",
    "    # \"_tabular_log_reg_onehot\": 'LogR (One-Hot)',\n",
    "    'cnn': '1D-CNN + MLP',\n",
    "    '_tabular_mlp_onehot': 'MLP (One-Hot)',\n",
    "    '_tabular_xgb_onehot': 'GBDT (One-Hot)',\n",
    "    'mlp_seq': 'MLP (Embedding)',\n",
    "    # 'neurlux': '1D-CNN + LSTM\\n+ Attention + MLP',\n",
    "    'cls_transformer': 'Transformer (CLS)',\n",
    "    # 'mean_transformer': 'Transformer\\n(Mean Pooling)',\n",
    "    #'_tabular_mlp_minhash': 'MLP (Minhash)',\n",
    "    # 'lstm': 'LSTM + MLP',\n",
    "    # 'cnn_lstm': '1D-CNN + LSTM\\n+ MLP',\n",
    "    #'attpool_transformer': 'Transformer\\n(Attent. Pooling)',\n",
    "}\n",
    "for name in to_keep.keys():\n",
    "    y_pred_proba = y_preds[name]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "    name = to_keep[name]\n",
    "    plot_roc_curve(fpr, tpr, None, model_name=name, ax=ax, semilogx=True, xlim=[2e-6,1e-2], ylim=[0.5,1.05])\n",
    "    plot_roc_curve(fpr, tpr, None, model_name='', ax=axin, semilogx=True, xlim=None, ylim=None)\n",
    "\n",
    "# Create a mapping from labels to handles\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "label_to_handle = {label: handle for handle, label in zip(handles, labels)}\n",
    "\n",
    "# labels for ROC curve\n",
    "ax.set_xlabel(\"False Positive Rate\", fontsize=20)\n",
    "#ax.set_ylabel(\"True Positive Rate\", fontsize=18)\n",
    "ax.set_title(\"Test set ROC curves\", fontsize=18)\n",
    "\n",
    "# ticklbel fontsizes to 18 as well\n",
    "ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "# Order the handles based on the order of labels in to_keep.values()\n",
    "new_handles = [label_to_handle[label] for label in to_keep.values()]\n",
    "new_labels = list(to_keep.values())\n",
    "ax.grid(linewidth=0.2)\n",
    "ax.legend(new_handles, new_labels, ncol=1, fontsize=16, bbox_to_anchor=(1.03, 0), loc='lower right')\n",
    "\n",
    "axin.set_xlim([3e-6, 2e-5])\n",
    "axin.set_ylim([0.95, 1.01])\n",
    "ax.indicate_inset_zoom(axin)\n",
    "\n",
    "axin.set_yticks([])\n",
    "axin.set_xticklabels([])\n",
    "axin.set_yticklabels([])\n",
    "axin.set_xticks([], minor=True)\n",
    "axin.xaxis.set_ticks_position('none') \n",
    "\n",
    "# save as pdf in \"/img\"\n",
    "fig.savefig(f\"img/roc_ablation_models.pdf\", bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1285559/420440982.py:53: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, new_row], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>TPR at FPR=10^-7</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_tabular_mlp_onehot</td>\n",
       "      <td>81.14</td>\n",
       "      <td>99.95</td>\n",
       "      <td>99.95</td>\n",
       "      <td>100.00</td>\n",
       "      <td>99.96</td>\n",
       "      <td>99.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_tabular_rf_onehot</td>\n",
       "      <td>41.08</td>\n",
       "      <td>98.91</td>\n",
       "      <td>98.90</td>\n",
       "      <td>99.82</td>\n",
       "      <td>99.93</td>\n",
       "      <td>97.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_tabular_xgb_onehot</td>\n",
       "      <td>68.93</td>\n",
       "      <td>99.85</td>\n",
       "      <td>99.85</td>\n",
       "      <td>99.98</td>\n",
       "      <td>99.91</td>\n",
       "      <td>99.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mlp_seq</td>\n",
       "      <td>36.36</td>\n",
       "      <td>88.56</td>\n",
       "      <td>89.42</td>\n",
       "      <td>88.79</td>\n",
       "      <td>81.94</td>\n",
       "      <td>96.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>attpool_transformer</td>\n",
       "      <td>0.00</td>\n",
       "      <td>97.81</td>\n",
       "      <td>97.76</td>\n",
       "      <td>99.64</td>\n",
       "      <td>99.65</td>\n",
       "      <td>96.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cls_transformer</td>\n",
       "      <td>0.00</td>\n",
       "      <td>99.83</td>\n",
       "      <td>99.83</td>\n",
       "      <td>99.99</td>\n",
       "      <td>99.74</td>\n",
       "      <td>99.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mean_transformer</td>\n",
       "      <td>0.00</td>\n",
       "      <td>94.18</td>\n",
       "      <td>94.43</td>\n",
       "      <td>98.19</td>\n",
       "      <td>90.04</td>\n",
       "      <td>98.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neurlux</td>\n",
       "      <td>37.23</td>\n",
       "      <td>98.91</td>\n",
       "      <td>98.89</td>\n",
       "      <td>99.80</td>\n",
       "      <td>99.88</td>\n",
       "      <td>97.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cnn</td>\n",
       "      <td>91.26</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lstm</td>\n",
       "      <td>36.36</td>\n",
       "      <td>99.79</td>\n",
       "      <td>99.79</td>\n",
       "      <td>99.93</td>\n",
       "      <td>100.00</td>\n",
       "      <td>99.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cnn_lstm</td>\n",
       "      <td>42.54</td>\n",
       "      <td>99.15</td>\n",
       "      <td>99.14</td>\n",
       "      <td>99.92</td>\n",
       "      <td>99.93</td>\n",
       "      <td>98.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cnn_non_augm</td>\n",
       "      <td>0.00</td>\n",
       "      <td>80.29</td>\n",
       "      <td>77.91</td>\n",
       "      <td>87.44</td>\n",
       "      <td>89.98</td>\n",
       "      <td>72.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  TPR at FPR=10^-7  F1-Score  Accuracy     AUC  Recall  \\\n",
       "0   _tabular_mlp_onehot             81.14     99.95     99.95  100.00   99.96   \n",
       "1    _tabular_rf_onehot             41.08     98.91     98.90   99.82   99.93   \n",
       "2   _tabular_xgb_onehot             68.93     99.85     99.85   99.98   99.91   \n",
       "3               mlp_seq             36.36     88.56     89.42   88.79   81.94   \n",
       "4   attpool_transformer              0.00     97.81     97.76   99.64   99.65   \n",
       "5       cls_transformer              0.00     99.83     99.83   99.99   99.74   \n",
       "6      mean_transformer              0.00     94.18     94.43   98.19   90.04   \n",
       "7               neurlux             37.23     98.91     98.89   99.80   99.88   \n",
       "8                   cnn             91.26    100.00    100.00  100.00  100.00   \n",
       "9                  lstm             36.36     99.79     99.79   99.93  100.00   \n",
       "10             cnn_lstm             42.54     99.15     99.14   99.92   99.93   \n",
       "11         cnn_non_augm              0.00     80.29     77.91   87.44   89.98   \n",
       "\n",
       "    Precision  \n",
       "0       99.95  \n",
       "1       97.91  \n",
       "2       99.80  \n",
       "3       96.35  \n",
       "4       96.03  \n",
       "5       99.92  \n",
       "6       98.71  \n",
       "7       97.95  \n",
       "8      100.00  \n",
       "9       99.59  \n",
       "10      98.38  \n",
       "11      72.48  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "\n",
    "def get_tpr_at_fpr(predicted_logits, true_labels, fprNeeded=1e-5):\n",
    "    predicted_probs = torch.sigmoid(predicted_logits).cpu().detach().numpy()\n",
    "    fpr, tpr, _ = roc_curve(true_labels, predicted_probs)\n",
    "    if all(np.isnan(fpr)):\n",
    "        return np.nan#, np.nan\n",
    "    else:\n",
    "        tpr_at_fpr = tpr[fpr <= fprNeeded][-1]\n",
    "        return tpr_at_fpr\n",
    "\n",
    "results_df = pd.DataFrame(columns=['Model', 'TPR at FPR=10^-5', 'F1-Score', 'Precision', 'Recall', 'AUC', 'Accuracy'])\n",
    "\n",
    "\n",
    "def get_threshold_with_highest_f1(predictions, true_labels):\n",
    "    fpr, tpr, thresholds = roc_curve(true_labels, predictions)\n",
    "    f1_scores = 2 * tpr * (1 - fpr)\n",
    "    best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "    return best_threshold\n",
    "\n",
    "\n",
    "# Loop through each key-value pair in your dictionary, compute the required metrics, and append them to results_df\n",
    "for model_name, predictions in y_preds.items():\n",
    "    predicted_probs = torch.sigmoid(torch.tensor(predictions)).cpu().detach().numpy()\n",
    "    best_threshold = get_threshold_with_highest_f1(predicted_probs, y_test)\n",
    "    binary_preds = (predicted_probs > best_threshold).astype(int)  # Convert probabilities to binary labels with a 0.05 threshold\n",
    "    \n",
    "    auc = round(roc_auc_score(y_test, predicted_probs) * 100, 2)\n",
    "    f1 = round(f1_score(y_test, binary_preds) * 100, 2)\n",
    "    precision = round(precision_score(y_test, binary_preds) * 100, 2)\n",
    "    recall = round(recall_score(y_test, binary_preds) * 100, 2)\n",
    "    accuracy = round(accuracy_score(y_test, binary_preds) * 100, 2)\n",
    "    tpr_at_fpr_1e4 = round(get_tpr_at_fpr(torch.tensor(predictions), y_test, fprNeeded=1e-4) * 100, 2)\n",
    "    tpr_at_fpr_1e5 = round(get_tpr_at_fpr(torch.tensor(predictions), y_test, fprNeeded=1e-5) * 100, 2)\n",
    "    tpr_at_fpr_1e6 = round(get_tpr_at_fpr(torch.tensor(predictions), y_test, fprNeeded=1e-6) * 100, 2)\n",
    "    tpr_at_fpr_1e7 = round(get_tpr_at_fpr(torch.tensor(predictions), y_test, fprNeeded=1e-7) * 100, 2)\n",
    "    \n",
    "    # Append results to dataframe\n",
    "    new_row = pd.DataFrame({\n",
    "        'Model': [model_name],\n",
    "        'TPR at FPR=10^-7': [tpr_at_fpr_1e7],\n",
    "        'TPR at FPR=10^-6': [tpr_at_fpr_1e6],\n",
    "        'TPR at FPR=10^-5': [tpr_at_fpr_1e5],\n",
    "        'TPR at FPR=10^-4': [tpr_at_fpr_1e4],\n",
    "        'F1-Score': [f1],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall],\n",
    "        'Accuracy': [accuracy],\n",
    "        'AUC': [auc]\n",
    "    })\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "# Output the results\n",
    "# results_df[['Model', 'TPR at FPR=10^-7', 'TPR at FPR=10^-6', 'TPR at FPR=10^-5', 'TPR at FPR=10^-4', 'F1-Score', 'Accuracy', 'AUC', 'Recall', 'Precision']]\n",
    "results_df[['Model', 'TPR at FPR=10^-7', 'F1-Score', 'Accuracy', 'AUC', 'Recall', 'Precision']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scienceplots\n",
    "plt.style.use(['science', 'no-latex'])\n",
    "plt.rc('text', usetex=True)\n",
    "\n",
    "from src.plots import set_size\n",
    "\n",
    "df_dict = {}\n",
    "LOGDIR = 'logs_models'\n",
    "\n",
    "# key_extractor = lambda x: x.split('_')[1:]\n",
    "\n",
    "for subfolder in os.listdir(LOGDIR):\n",
    "    subfolder_path = os.path.join(LOGDIR, subfolder)\n",
    "    csv_file_path = os.path.join(subfolder_path, 'version_0', 'metrics.csv')\n",
    "    if os.path.exists(csv_file_path):\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        encoder = ' '.join(subfolder.split('_')[:-1]).title()\n",
    "        df_dict[encoder] = df\n",
    "\n",
    "\n",
    "def extract_metric_values(df_dict, metric_name, operation='last'):\n",
    "    \"\"\"\n",
    "    Extracts specified metric values (either last or max) from nested DataFrames in df_dict.\n",
    "    \n",
    "    Parameters:\n",
    "        df_dict (defaultdict(dict)): Nested dictionary of DataFrames keyed by tokenizer and vocab_size.\n",
    "        metric_name (str): The name of the metric to extract.\n",
    "        operation (str): The operation to perform ('last' or 'max').\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame containing the extracted metric values, indexed by vocab_size and columns by tokenizer.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to temporarily store the extracted values\n",
    "    extracted_values_temp = {}\n",
    "    \n",
    "    # Loop through the nested DataFrames\n",
    "    for encoder, df in df_dict.items():\n",
    "        if metric_name in df.columns:\n",
    "            if operation == 'last':\n",
    "                # Extract the last value of the specified metric\n",
    "                last_value = df[metric_name].dropna().iloc[-1]\n",
    "                extracted_values_temp[encoder] = last_value\n",
    "                \n",
    "            elif operation == 'max':\n",
    "                # Extract the maximum value of the specified metric\n",
    "                max_value = df[metric_name].dropna().max()\n",
    "                extracted_values_temp[encoder] = max_value\n",
    "            else:\n",
    "                print(f\"Invalid operation '{operation}' specified. Skipping {encoder}.\")\n",
    "        else:\n",
    "            print(f\"Metric '{metric_name}' not found in DataFrame for {encoder}. Skipping.\")\n",
    "                \n",
    "    # Convert the nested dictionary to a DataFrame, using 'metric_name' as value for index\n",
    "    extracted_values_df = pd.DataFrame.from_dict(extracted_values_temp, orient='index', columns=[metric_name]).transpose()\n",
    "\n",
    "    # sort index by vocab_size\n",
    "    extracted_values_df.sort_index(inplace=True)\n",
    "\n",
    "    return extracted_values_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_tpr</th>\n",
       "      <th>val_f1</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Attpool Transformer</th>\n",
       "      <td>0.967858</td>\n",
       "      <td>0.982970</td>\n",
       "      <td>0.983254</td>\n",
       "      <td>0.999406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cls Transformer</th>\n",
       "      <td>0.997018</td>\n",
       "      <td>0.995994</td>\n",
       "      <td>0.996010</td>\n",
       "      <td>0.999996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cnn</th>\n",
       "      <td>0.999910</td>\n",
       "      <td>0.974444</td>\n",
       "      <td>0.975081</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cnn Lstm</th>\n",
       "      <td>0.780494</td>\n",
       "      <td>0.642088</td>\n",
       "      <td>0.736428</td>\n",
       "      <td>0.996596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lstm</th>\n",
       "      <td>0.972996</td>\n",
       "      <td>0.940016</td>\n",
       "      <td>0.943411</td>\n",
       "      <td>0.999938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean Transformer</th>\n",
       "      <td>0.967348</td>\n",
       "      <td>0.994659</td>\n",
       "      <td>0.994687</td>\n",
       "      <td>0.999963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mlp Seq</th>\n",
       "      <td>0.969886</td>\n",
       "      <td>0.917868</td>\n",
       "      <td>0.924095</td>\n",
       "      <td>0.999945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neurlux</th>\n",
       "      <td>0.968056</td>\n",
       "      <td>0.900832</td>\n",
       "      <td>0.909776</td>\n",
       "      <td>0.999922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tabular Mlp Minhash</th>\n",
       "      <td>0.993898</td>\n",
       "      <td>0.986445</td>\n",
       "      <td>0.986625</td>\n",
       "      <td>0.999988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tabular Mlp Onehot</th>\n",
       "      <td>0.999303</td>\n",
       "      <td>0.993978</td>\n",
       "      <td>0.994014</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Log Reg Minhash</th>\n",
       "      <td>0.778231</td>\n",
       "      <td>0.925402</td>\n",
       "      <td>0.930557</td>\n",
       "      <td>0.999818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Log Reg Onehot</th>\n",
       "      <td>0.998843</td>\n",
       "      <td>0.999268</td>\n",
       "      <td>0.999268</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rf Minhash</th>\n",
       "      <td>0.790049</td>\n",
       "      <td>0.995738</td>\n",
       "      <td>0.995720</td>\n",
       "      <td>0.999948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rf Onehot</th>\n",
       "      <td>0.952684</td>\n",
       "      <td>0.993455</td>\n",
       "      <td>0.993412</td>\n",
       "      <td>0.999982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Xgb Minhash</th>\n",
       "      <td>0.963397</td>\n",
       "      <td>0.999381</td>\n",
       "      <td>0.999381</td>\n",
       "      <td>0.999944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Xgb Onehot</th>\n",
       "      <td>0.999426</td>\n",
       "      <td>0.999657</td>\n",
       "      <td>0.999658</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       val_tpr    val_f1   val_acc   val_auc\n",
       "Attpool Transformer   0.967858  0.982970  0.983254  0.999406\n",
       "Cls Transformer       0.997018  0.995994  0.996010  0.999996\n",
       "Cnn                   0.999910  0.974444  0.975081  1.000000\n",
       "Cnn Lstm              0.780494  0.642088  0.736428  0.996596\n",
       "Lstm                  0.972996  0.940016  0.943411  0.999938\n",
       "Mean Transformer      0.967348  0.994659  0.994687  0.999963\n",
       "Mlp Seq               0.969886  0.917868  0.924095  0.999945\n",
       "Neurlux               0.968056  0.900832  0.909776  0.999922\n",
       " Tabular Mlp Minhash  0.993898  0.986445  0.986625  0.999988\n",
       " Tabular Mlp Onehot   0.999303  0.993978  0.994014  0.999999\n",
       "Log Reg Minhash       0.778231  0.925402  0.930557  0.999818\n",
       "Log Reg Onehot        0.998843  0.999268  0.999268  1.000000\n",
       "Rf Minhash            0.790049  0.995738  0.995720  0.999948\n",
       "Rf Onehot             0.952684  0.993455  0.993412  0.999982\n",
       "Xgb Minhash           0.963397  0.999381  0.999381  0.999944\n",
       "Xgb Onehot            0.999426  0.999657  0.999658  1.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = ['val_tpr', 'val_f1', 'val_acc', 'val_auc']\n",
    "df = pd.DataFrame()\n",
    "for metric in metrics:\n",
    "    df1 = extract_metric_values(df_dict, metric, operation='last')\n",
    "    df = pd.concat([df, df1], axis=0)\n",
    "\n",
    "tabular_df_dict = {}\n",
    "for subfolder in os.listdir(LOGDIR):\n",
    "    subfolder_path = os.path.join(LOGDIR, subfolder)\n",
    "    csv_file_path = os.path.join(subfolder_path, 'metrics.csv')\n",
    "    if os.path.exists(csv_file_path):\n",
    "        ldf = pd.read_csv(csv_file_path)\n",
    "        encoder = ' '.join(subfolder.split('_')[2:]).title()\n",
    "        tabular_df_dict[encoder] = ldf\n",
    "\n",
    "col_map = dict(zip(['tpr', 'f1', 'acc', 'auc'], ['val_tpr', 'val_f1', 'val_acc', 'val_auc']))\n",
    "\n",
    "# Iterate over each item in the dictionary, map the columns, and concatenate to the initial dataframe\n",
    "for model_name, ldf in tabular_df_dict.items():\n",
    "    # Map columns\n",
    "    ldf = ldf.rename(columns=col_map)\n",
    "    # Transpose the dataframe so that metrics are in the columns and model names are in the index\n",
    "    ldf = ldf.T\n",
    "    # Rename the columns of df to match the model name\n",
    "    ldf.columns = [model_name]\n",
    "    # Concatenate along columns axis\n",
    "    df = pd.concat([df, ldf], axis=1)\n",
    "\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_tpr</th>\n",
       "      <th>val_f1</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Attpool Transformer</th>\n",
       "      <td>0.983769</td>\n",
       "      <td>0.999551</td>\n",
       "      <td>0.999551</td>\n",
       "      <td>0.999963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cls Transformer</th>\n",
       "      <td>0.997021</td>\n",
       "      <td>0.999889</td>\n",
       "      <td>0.999889</td>\n",
       "      <td>0.999996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cnn</th>\n",
       "      <td>0.999910</td>\n",
       "      <td>0.999949</td>\n",
       "      <td>0.999949</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cnn Lstm</th>\n",
       "      <td>0.916991</td>\n",
       "      <td>0.986644</td>\n",
       "      <td>0.986585</td>\n",
       "      <td>0.997550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lstm</th>\n",
       "      <td>0.992145</td>\n",
       "      <td>0.967622</td>\n",
       "      <td>0.968636</td>\n",
       "      <td>0.999983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean Transformer</th>\n",
       "      <td>0.992789</td>\n",
       "      <td>0.999902</td>\n",
       "      <td>0.999902</td>\n",
       "      <td>0.999988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mlp Seq</th>\n",
       "      <td>0.980032</td>\n",
       "      <td>0.990541</td>\n",
       "      <td>0.990622</td>\n",
       "      <td>0.999957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neurlux</th>\n",
       "      <td>0.980555</td>\n",
       "      <td>0.985019</td>\n",
       "      <td>0.984945</td>\n",
       "      <td>0.999957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tabular Mlp Minhash</th>\n",
       "      <td>0.994517</td>\n",
       "      <td>0.999298</td>\n",
       "      <td>0.999298</td>\n",
       "      <td>0.999989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tabular Mlp Onehot</th>\n",
       "      <td>0.999554</td>\n",
       "      <td>0.999777</td>\n",
       "      <td>0.999777</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Log Reg Minhash</th>\n",
       "      <td>0.778231</td>\n",
       "      <td>0.925402</td>\n",
       "      <td>0.930557</td>\n",
       "      <td>0.999818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Log Reg Onehot</th>\n",
       "      <td>0.998843</td>\n",
       "      <td>0.999268</td>\n",
       "      <td>0.999268</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rf Minhash</th>\n",
       "      <td>0.790049</td>\n",
       "      <td>0.995738</td>\n",
       "      <td>0.995720</td>\n",
       "      <td>0.999948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rf Onehot</th>\n",
       "      <td>0.952684</td>\n",
       "      <td>0.993455</td>\n",
       "      <td>0.993412</td>\n",
       "      <td>0.999982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Xgb Minhash</th>\n",
       "      <td>0.963397</td>\n",
       "      <td>0.999381</td>\n",
       "      <td>0.999381</td>\n",
       "      <td>0.999944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Xgb Onehot</th>\n",
       "      <td>0.999426</td>\n",
       "      <td>0.999657</td>\n",
       "      <td>0.999658</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       val_tpr    val_f1   val_acc   val_auc\n",
       "Attpool Transformer   0.983769  0.999551  0.999551  0.999963\n",
       "Cls Transformer       0.997021  0.999889  0.999889  0.999996\n",
       "Cnn                   0.999910  0.999949  0.999949  1.000000\n",
       "Cnn Lstm              0.916991  0.986644  0.986585  0.997550\n",
       "Lstm                  0.992145  0.967622  0.968636  0.999983\n",
       "Mean Transformer      0.992789  0.999902  0.999902  0.999988\n",
       "Mlp Seq               0.980032  0.990541  0.990622  0.999957\n",
       "Neurlux               0.980555  0.985019  0.984945  0.999957\n",
       " Tabular Mlp Minhash  0.994517  0.999298  0.999298  0.999989\n",
       " Tabular Mlp Onehot   0.999554  0.999777  0.999777  0.999999\n",
       "Log Reg Minhash       0.778231  0.925402  0.930557  0.999818\n",
       "Log Reg Onehot        0.998843  0.999268  0.999268  1.000000\n",
       "Rf Minhash            0.790049  0.995738  0.995720  0.999948\n",
       "Rf Onehot             0.952684  0.993455  0.993412  0.999982\n",
       "Xgb Minhash           0.963397  0.999381  0.999381  0.999944\n",
       "Xgb Onehot            0.999426  0.999657  0.999658  1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = ['val_tpr', 'val_f1', 'val_acc', 'val_auc']\n",
    "df = pd.DataFrame()\n",
    "for metric in metrics:\n",
    "    df1 = extract_metric_values(df_dict, metric, operation='max')\n",
    "    df = pd.concat([df, df1], axis=0)\n",
    "\n",
    "tabular_df_dict = {}\n",
    "for subfolder in os.listdir(LOGDIR):\n",
    "    subfolder_path = os.path.join(LOGDIR, subfolder)\n",
    "    csv_file_path = os.path.join(subfolder_path, 'metrics.csv')\n",
    "    if os.path.exists(csv_file_path):\n",
    "        ldf = pd.read_csv(csv_file_path)\n",
    "        encoder = ' '.join(subfolder.split('_')[2:]).title()\n",
    "        tabular_df_dict[encoder] = ldf\n",
    "\n",
    "col_map = dict(zip(['tpr', 'f1', 'acc', 'auc'], ['val_tpr', 'val_f1', 'val_acc', 'val_auc']))\n",
    "\n",
    "# Iterate over each item in the dictionary, map the columns, and concatenate to the initial dataframe\n",
    "for model_name, ldf in tabular_df_dict.items():\n",
    "    # Map columns\n",
    "    ldf = ldf.rename(columns=col_map)\n",
    "    # Transpose the dataframe so that metrics are in the columns and model names are in the index\n",
    "    ldf = ldf.T\n",
    "    # Rename the columns of df to match the model name\n",
    "    ldf.columns = [model_name]\n",
    "    # Concatenate along columns axis\n",
    "    df = pd.concat([df, ldf], axis=1)\n",
    "\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
