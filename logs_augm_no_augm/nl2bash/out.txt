C:\Users\dtrizna\Code\Synapse\Linux>python3 ablation_nl2bash_augm_no_augm.py
Python implementation: CPython
Python version       : 3.9.13
IPython version      : 8.11.0

torch    : 2.0.1+cu117
lightning: 1.8.6
sklearn  : 0.0.post1

[!] Script start time: Thu Jan 18 07:09:41 2024
Global seed set to 33
[!] Generating 294 number of examples per template.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:00<00:00, 149.43it/s]
[!] Generated total 6174 commands.
[!] Generating 1 number of examples per template.
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:00<?, ?it/s]
[!] Generated total 21 commands.
[!] Generating 1 number of examples per template.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<?, ?it/s] 
[!] Generated total 9 commands.
[!] Generated 14998 augmented train commands.
[!] Generated 8845 non-augmented train commands.
[!] Generated 3792 test commands (imbalanced).
    Size of nl2bash train set: 8824
    Size of nl2bash test set: 3783
[*] Building vocab and encoding...
[*] Fitting One-Hot encoder...
[*] Building vocab and encoding...
[*] Fitting One-Hot encoder...
[*] Training augm_xgb_onehot... Started: Thu Jan 18 07:09:44 2024
[*] Training augm_xgb_onehot model...
[!] augm_xgb_onehot model scores: train_tpr=1.0000, train_f1=1.0000, train_acc=1.0000, train_auc=1.0000
[!] augm_xgb_onehot model scores: val_tpr=0.8889, val_f1=0.9000, val_acc=0.9995, val_auc=1.0000
[*] Training not_augm_xgb_onehot model...
[!] not_augm_xgb_onehot model scores: train_tpr=1.0000, train_f1=1.0000, train_acc=1.0000, train_auc=1.0000
[!] not_augm_xgb_onehot model scores: val_tpr=0.2222, val_f1=0.2000, val_acc=0.9979, val_auc=0.8486
[*] Training augm_mlp_onehot... Started: Thu Jan 18 07:09:45 2024
[!] Scheduler: onecycle | Scheduler step budget: 300
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
C:\Users\dtrizna\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\lightning\pytorch\trainer\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.
  rank_zero_warn(
[*] Training 'augm_mlp_onehot' model...

   | Name      | Type              | Params
-------------------------------------------------
0  | model     | SimpleMLP         | 264 K
1  | loss      | BCEWithLogitsLoss | 0
2  | train_acc | BinaryAccuracy    | 0
3  | train_f1  | BinaryF1Score     | 0
4  | train_auc | BinaryAUROC       | 0
5  | val_acc   | BinaryAccuracy    | 0
6  | val_f1    | BinaryF1Score     | 0
7  | val_auc   | BinaryAUROC       | 0
8  | test_acc  | BinaryAccuracy    | 0
9  | test_f1   | BinaryF1Score     | 0
10 | test_auc  | BinaryAUROC       | 0
-------------------------------------------------
264 K     Trainable params
0         Non-trainable params
264 K     Total params
1.057     Total estimated model params size (MB)
Epoch 0:  48%|████████████████████████████████████████████████████████████████                                                                      | 11/23 [00:44<00:48,  4.01s/it, loss=0.684, v_num=0, train_loss=0.684, val_acc=0.998, val_f1=0.000, val_auc=0.701, val_tpr=0.0949]Metric val_tpr improved. New best score: 0.095
Epoch 0:  96%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏     | 22/23 [00:44<00:02,  2.02s/it, loss=0.682, v_num=0, train_loss=0.683, val_acc=0.998, val_f1=0.000, val_auc=0.887, val_tpr=0.432]Metric val_tpr improved by 0.338 >= min_delta = 0.0001. New best score: 0.432
Epoch 0: 100%|██████████████████████████████████████████████████████████████████| 23/23 [00:44<00:00,  1.94s/it, loss=0.682, v_num=0, train_loss=0.683, val_acc=0.998, val_f1=0.000, val_auc=0.887, val_tpr=0.432, train_acc=0.588, train_f1=0.000, train_auc=0.534, train_tpr=0.00436]
Epoch 1:  48%|███████████████████████████████▌                                  | 11/23 [00:00<00:00, 15.22it/s, loss=0.681, v_num=0, train_loss=0.676, val_acc=0.998, val_f1=0.000, val_auc=0.994, val_tpr=0.568, train_acc=0.588, train_f1=0.000, train_auc=0.534, train_tpr=0.00436]Metric val_tpr improved by 0.135 >= min_delta = 0.0001. New best score: 0.568
Epoch 1:  96%|███████████████████████████████████████████████████████████████▏  | 22/23 [00:01<00:00, 20.43it/s, loss=0.677, v_num=0, train_loss=0.670, val_acc=0.998, val_f1=0.000, val_auc=1.000, val_tpr=1.000, train_acc=0.588, train_f1=0.000, train_auc=0.534, train_tpr=0.00436]Metric val_tpr improved by 0.432 >= min_delta = 0.0001. New best score: 1.000
Epoch 1: 100%|███████████████████████████████████████████████████████████████████| 23/23 [00:01<00:00, 17.73it/s, loss=0.676, v_num=0, train_loss=0.667, val_acc=0.998, val_f1=0.000, val_auc=1.000, val_tpr=1.000, train_acc=0.595, train_f1=0.030, train_auc=0.733, train_tpr=0.0725]
Epoch 2: 100%|████████████████████████████████████████████████████████████████████| 23/23 [00:01<00:00, 21.35it/s, loss=0.648, v_num=0, train_loss=0.607, val_acc=0.999, val_f1=0.714, val_auc=1.000, val_tpr=1.000, train_acc=0.717, train_f1=0.477, train_auc=0.963, train_tpr=0.558] 
Epoch 3: 100%|████████████████████████████████████████████████████████████████████| 23/23 [00:01<00:00, 22.18it/s, loss=0.546, v_num=0, train_loss=0.422, val_acc=1.000, val_f1=0.941, val_auc=1.000, val_tpr=1.000, train_acc=0.937, train_f1=0.917, train_auc=1.000, train_tpr=0.969]
Epoch 4:  48%|████████████████████████████████▌                                   | 11/23 [00:00<00:00, 16.89it/s, loss=0.451, v_num=0, train_loss=0.292, val_acc=1.000, val_f1=0.941, val_auc=0.999, val_tpr=0.730, train_acc=0.937, train_f1=0.917, train_auc=1.000, train_tpr=0.969]Monitored metric val_tpr did not improve in the last 5 records. Best score: 1.000. Signaling Trainer to stop.
Epoch 4:  48%|████████████████████████████████▌                                   | 11/23 [00:00<00:00, 16.36it/s, loss=0.451, v_num=0, train_loss=0.292, val_acc=1.000, val_f1=0.941, val_auc=0.999, val_tpr=0.730, train_acc=0.995, train_f1=0.994, train_auc=1.000, train_tpr=1.000]
Epoch 4:  48%|████████████████████████████████▌                                   | 11/23 [00:00<00:00, 16.31it/s, loss=0.451, v_num=0, train_loss=0.292, val_acc=1.000, val_f1=0.941, val_auc=0.999, val_tpr=0.730, train_acc=0.995, train_f1=0.994, train_auc=1.000, train_tpr=1.000] 
[!] Training of augm_mlp_onehot ended:  Thu Jan 18 07:11:18 2024  | Took: 93.41 seconds
[*] Training augm_mean_transformer... Started: Thu Jan 18 07:11:18 2024
[!] Scheduler: onecycle | Scheduler step budget: 300
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
C:\Users\dtrizna\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\lightning\pytorch\trainer\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.
  rank_zero_warn(
[*] Training 'augm_mean_transformer' model...

   | Name      | Type                   | Params
------------------------------------------------------
0  | model     | MeanTransformerEncoder | 335 K
1  | loss      | BCEWithLogitsLoss      | 0
2  | train_acc | BinaryAccuracy         | 0
3  | train_f1  | BinaryF1Score          | 0
4  | train_auc | BinaryAUROC            | 0
5  | val_acc   | BinaryAccuracy         | 0
6  | val_f1    | BinaryF1Score          | 0
7  | val_auc   | BinaryAUROC            | 0
8  | test_acc  | BinaryAccuracy         | 0
9  | test_f1   | BinaryF1Score          | 0
10 | test_auc  | BinaryAUROC            | 0
------------------------------------------------------
335 K     Trainable params
0         Non-trainable params
335 K     Total params
1.341     Total estimated model params size (MB)
Epoch 0:  48%|████████████████████████████████████████████████████████████████▌                                                                      | 11/23 [00:45<00:49,  4.12s/it, loss=0.754, v_num=0, train_loss=0.743, val_acc=0.998, val_f1=0.000, val_auc=0.711, val_tpr=0.000]Metric val_tpr improved. New best score: 0.000                                                                                                                                                                                                                                           
Epoch 0:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████      | 22/23 [01:16<00:03,  3.48s/it, loss=0.75, v_num=0, train_loss=0.731, val_acc=0.998, val_f1=0.000, val_auc=0.792, val_tpr=0.270]Metric val_tpr improved by 0.270 >= min_delta = 0.0001. New best score: 0.270
Epoch 0: 100%|█████████████████████████████████████████████████████████████████| 23/23 [01:19<00:00,  3.44s/it, loss=0.749, v_num=0, train_loss=0.723, val_acc=0.998, val_f1=0.000, val_auc=0.792, val_tpr=0.270, train_acc=0.519, train_f1=0.420, train_auc=0.522, train_tpr=0.000654]
Epoch 1: 100%|█████████████████████████████████████████████████████████████████| 23/23 [01:03<00:00,  2.76s/it, loss=0.712, v_num=0, train_loss=0.701, val_acc=0.998, val_f1=0.000, val_auc=0.858, val_tpr=0.270, train_acc=0.551, train_f1=0.345, train_auc=0.570, train_tpr=0.000158] 
Epoch 2: 100%|██████████████████████████████████████████████████████████████████| 23/23 [01:01<00:00,  2.65s/it, loss=0.663, v_num=0, train_loss=0.644, val_acc=0.997, val_f1=0.000, val_auc=0.898, val_tpr=0.270, train_acc=0.605, train_f1=0.325, train_auc=0.636, train_tpr=0.00413] 
Epoch 3:  48%|███████████████████████████████▌                                  | 11/23 [00:28<00:30,  2.56s/it, loss=0.639, v_num=0, train_loss=0.614, val_acc=0.997, val_f1=0.167, val_auc=0.924, val_tpr=0.270, train_acc=0.605, train_f1=0.325, train_auc=0.636, train_tpr=0.00413]Monitored metric val_tpr did not improve in the last 5 records. Best score: 0.270. Signaling Trainer to stop.
Epoch 3:  48%|████████████████████████████████                                   | 11/23 [00:28<00:30,  2.56s/it, loss=0.639, v_num=0, train_loss=0.614, val_acc=0.997, val_f1=0.167, val_auc=0.924, val_tpr=0.270, train_acc=0.662, train_f1=0.439, train_auc=0.707, train_tpr=0.0269]
Epoch 3:  48%|████████████████████████████████                                   | 11/23 [00:28<00:30,  2.56s/it, loss=0.639, v_num=0, train_loss=0.614, val_acc=0.997, val_f1=0.167, val_auc=0.924, val_tpr=0.270, train_acc=0.662, train_f1=0.439, train_auc=0.707, train_tpr=0.0269] 
[!] Training of augm_mean_transformer ended:  Thu Jan 18 07:15:24 2024  | Took: 246.36 seconds
[*] Training augm_cnn... Started: Thu Jan 18 07:15:24 2024
[!] Scheduler: onecycle | Scheduler step budget: 300
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
C:\Users\dtrizna\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\lightning\pytorch\trainer\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.
  rank_zero_warn(
[*] Training 'augm_cnn' model...

   | Name      | Type              | Params
-------------------------------------------------
0  | model     | CNN1DGroupedModel | 301 K
1  | loss      | BCEWithLogitsLoss | 0
2  | train_acc | BinaryAccuracy    | 0
3  | train_f1  | BinaryF1Score     | 0
4  | train_auc | BinaryAUROC       | 0
5  | val_acc   | BinaryAccuracy    | 0
6  | val_f1    | BinaryF1Score     | 0
7  | val_auc   | BinaryAUROC       | 0
8  | test_acc  | BinaryAccuracy    | 0
9  | test_f1   | BinaryF1Score     | 0
10 | test_auc  | BinaryAUROC       | 0
-------------------------------------------------
301 K     Trainable params
0         Non-trainable params
301 K     Total params
1.205     Total estimated model params size (MB)
Epoch 0:  48%|████████████████████████████████████████████████████████████████▌                                                                      | 11/23 [00:15<00:17,  1.43s/it, loss=0.678, v_num=0, train_loss=0.677, val_acc=0.998, val_f1=0.000, val_auc=0.868, val_tpr=0.000]Metric val_tpr improved. New best score: 0.000
Epoch 0:  96%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏     | 22/23 [00:19<00:00,  1.15it/s, loss=0.675, v_num=0, train_loss=0.675, val_acc=0.998, val_f1=0.000, val_auc=0.963, val_tpr=0.135]Metric val_tpr improved by 0.135 >= min_delta = 0.0001. New best score: 0.135
Epoch 0: 100%|███████████████████████████████████████████████████████████████████| 23/23 [00:19<00:00,  1.19it/s, loss=0.675, v_num=0, train_loss=0.672, val_acc=0.998, val_f1=0.000, val_auc=0.963, val_tpr=0.135, train_acc=0.615, train_f1=0.406, train_auc=0.585, train_tpr=0.0147]
Epoch 1:  48%|████████████████████████████████▌                                   | 11/23 [00:03<00:03,  3.17it/s, loss=0.67, v_num=0, train_loss=0.655, val_acc=0.998, val_f1=0.000, val_auc=0.995, val_tpr=0.432, train_acc=0.615, train_f1=0.406, train_auc=0.585, train_tpr=0.0147]Metric val_tpr improved by 0.297 >= min_delta = 0.0001. New best score: 0.432
Epoch 1:  96%|████████████████████████████████████████████████████████████████   | 22/23 [00:06<00:00,  3.20it/s, loss=0.659, v_num=0, train_loss=0.634, val_acc=0.998, val_f1=0.364, val_auc=0.998, val_tpr=0.770, train_acc=0.615, train_f1=0.406, train_auc=0.585, train_tpr=0.0147]Metric val_tpr improved by 0.338 >= min_delta = 0.0001. New best score: 0.770
Epoch 1: 100%|███████████████████████████████████████████████████████████████████| 23/23 [00:07<00:00,  3.21it/s, loss=0.657, v_num=0, train_loss=0.630, val_acc=0.998, val_f1=0.364, val_auc=0.998, val_tpr=0.770, train_acc=0.686, train_f1=0.534, train_auc=0.721, train_tpr=0.0369]
Epoch 2:  48%|████████████████████████████████                                   | 11/23 [00:03<00:03,  3.36it/s, loss=0.632, v_num=0, train_loss=0.575, val_acc=0.999, val_f1=0.889, val_auc=1.000, val_tpr=0.865, train_acc=0.686, train_f1=0.534, train_auc=0.721, train_tpr=0.0369]Metric val_tpr improved by 0.095 >= min_delta = 0.0001. New best score: 0.865
Epoch 2:  96%|████████████████████████████████████████████████████████████████   | 22/23 [00:06<00:00,  3.35it/s, loss=0.585, v_num=0, train_loss=0.473, val_acc=1.000, val_f1=0.947, val_auc=1.000, val_tpr=1.000, train_acc=0.686, train_f1=0.534, train_auc=0.721, train_tpr=0.0369]Metric val_tpr improved by 0.135 >= min_delta = 0.0001. New best score: 1.000
Epoch 2: 100%|████████████████████████████████████████████████████████████████████| 23/23 [00:06<00:00,  3.36it/s, loss=0.576, v_num=0, train_loss=0.474, val_acc=1.000, val_f1=0.947, val_auc=1.000, val_tpr=1.000, train_acc=0.836, train_f1=0.778, train_auc=0.919, train_tpr=0.257]
Epoch 3: 100%|████████████████████████████████████████████████████████████████████| 23/23 [00:06<00:00,  3.39it/s, loss=0.331, v_num=0, train_loss=0.125, val_acc=0.992, val_f1=0.367, val_auc=1.000, val_tpr=1.000, train_acc=0.948, train_f1=0.935, train_auc=0.988, train_tpr=0.521] 
Epoch 4: 100%|██████████████████████████████████████████████████████████████████| 23/23 [00:06<00:00,  3.48it/s, loss=0.0777, v_num=0, train_loss=0.0261, val_acc=0.996, val_f1=0.514, val_auc=1.000, val_tpr=1.000, train_acc=0.989, train_f1=0.987, train_auc=0.999, train_tpr=0.794]
Epoch 5:  48%|███████████████████████████████▌                                  | 11/23 [00:03<00:03,  3.46it/s, loss=0.0335, v_num=0, train_loss=0.0129, val_acc=0.998, val_f1=0.692, val_auc=1.000, val_tpr=1.000, train_acc=0.989, train_f1=0.987, train_auc=0.999, train_tpr=0.794]Monitored metric val_tpr did not improve in the last 5 records. Best score: 1.000. Signaling Trainer to stop.
Epoch 5:  48%|███████████████████████████████▌                                  | 11/23 [00:03<00:03,  3.43it/s, loss=0.0335, v_num=0, train_loss=0.0129, val_acc=0.998, val_f1=0.692, val_auc=1.000, val_tpr=1.000, train_acc=0.997, train_f1=0.996, train_auc=1.000, train_tpr=0.975]
Epoch 5:  48%|███████████████████████████████▌                                  | 11/23 [00:03<00:03,  3.43it/s, loss=0.0335, v_num=0, train_loss=0.0129, val_acc=0.998, val_f1=0.692, val_auc=1.000, val_tpr=1.000, train_acc=0.997, train_f1=0.996, train_auc=1.000, train_tpr=0.975]
[!] Training of augm_cnn ended:  Thu Jan 18 07:16:29 2024  | Took: 64.22 seconds
[*] Training not_augm_xgb_onehot... Started: Thu Jan 18 07:16:29 2024
[*] Training not_augm_xgb_onehot model...
[!] not_augm_xgb_onehot model scores: tpr=0.2222, f1=0.2000, acc=0.9979, auc=0.8486
[!] Training of not_augm_xgb_onehot ended:  Thu Jan 18 07:16:29 2024  | Took: 0.10 seconds
[*] Training not_augm_mlp_onehot... Started: Thu Jan 18 07:16:29 2024
[!] Scheduler: onecycle | Scheduler step budget: 180
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
C:\Users\dtrizna\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\lightning\pytorch\trainer\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.
  rank_zero_warn(
[*] Training 'not_augm_mlp_onehot' model...

   | Name      | Type              | Params
-------------------------------------------------
0  | model     | SimpleMLP         | 264 K
1  | loss      | BCEWithLogitsLoss | 0
2  | train_acc | BinaryAccuracy    | 0
3  | train_f1  | BinaryF1Score     | 0
4  | train_auc | BinaryAUROC       | 0
5  | val_acc   | BinaryAccuracy    | 0
6  | val_f1    | BinaryF1Score     | 0
7  | val_auc   | BinaryAUROC       | 0
8  | test_acc  | BinaryAccuracy    | 0
9  | test_f1   | BinaryF1Score     | 0
10 | test_auc  | BinaryAUROC       | 0
-------------------------------------------------
264 K     Trainable params
0         Non-trainable params
264 K     Total params
1.057     Total estimated model params size (MB)
C:\Users\dtrizna\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\lightning\pytorch\trainer\trainer.py:1595: PossibleUserWarning: The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
Epoch 0:  47%|██████████████████████████████████████████████████████████████▌                                                                      | 8/17 [00:39<00:44,  4.98s/it, loss=0.696, v_num=0, train_loss=0.696, val_acc=0.0512, val_f1=0.00388, val_auc=0.362, val_tpr=0.000]Metric val_tpr improved. New best score: 0.000
Epoch 0:  94%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏       | 16/17 [00:40<00:02,  2.51s/it, loss=0.696, v_num=0, train_loss=0.695, val_acc=0.187, val_f1=0.00323, val_auc=0.317, val_tpr=0.000]C:\Users\dtrizna\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torchmetrics\utilities\prints.py:42: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
  warnings.warn(*args, **kwargs)  # noqa: B028
C:\Users\dtrizna\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\sklearn\metrics\_ranking.py:1016: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless
  warnings.warn(
Epoch 0: 100%|████████████████████████████████████████████████████████████████| 17/17 [00:40<00:00,  2.36s/it, loss=0.696, v_num=0, train_loss=0.694, val_acc=0.187, val_f1=0.00323, val_auc=0.317, val_tpr=0.000, train_acc=0.447, train_f1=0.00366, train_auc=0.443, train_tpr=nan.0] 
NaN or Inf found in input tensor.
Epoch 1:  94%|██████████████████████████████████████████████████████████████    | 16/17 [00:00<00:00, 19.32it/s, loss=0.694, v_num=0, train_loss=0.689, val_acc=0.977, val_f1=0.000, val_auc=0.274, val_tpr=0.000, train_acc=0.447, train_f1=0.00366, train_auc=0.443, train_tpr=nan.0]C:\Users\dtrizna\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torchmetrics\utilities\prints.py:42: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
  warnings.warn(*args, **kwargs)  # noqa: B028
C:\Users\dtrizna\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\sklearn\metrics\_ranking.py:1016: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless
  warnings.warn(
Epoch 1: 100%|██████████████████████████████████████████████████████████████████| 17/17 [00:00<00:00, 19.90it/s, loss=0.693, v_num=0, train_loss=0.687, val_acc=0.977, val_f1=0.000, val_auc=0.274, val_tpr=0.000, train_acc=0.533, train_f1=0.00482, train_auc=0.470, train_tpr=nan.0] 
NaN or Inf found in input tensor.
Epoch 2:  94%|██████████████████████████████████████████████████████████████    | 16/17 [00:00<00:00, 19.89it/s, loss=0.687, v_num=0, train_loss=0.670, val_acc=0.998, val_f1=0.000, val_auc=0.306, val_tpr=0.000, train_acc=0.533, train_f1=0.00482, train_auc=0.470, train_tpr=nan.0]Monitored metric val_tpr did not improve in the last 5 records. Best score: 0.000. Signaling Trainer to stop.
Epoch 2:  94%|██████████████████████████████████████████████████████████████    | 16/17 [00:00<00:00, 19.18it/s, loss=0.687, v_num=0, train_loss=0.670, val_acc=0.998, val_f1=0.000, val_auc=0.306, val_tpr=0.000, train_acc=0.744, train_f1=0.00286, train_auc=0.302, train_tpr=0.000]
Epoch 2:  94%|██████████████████████████████████████████████████████████████    | 16/17 [00:00<00:00, 18.98it/s, loss=0.687, v_num=0, train_loss=0.670, val_acc=0.998, val_f1=0.000, val_auc=0.306, val_tpr=0.000, train_acc=0.744, train_f1=0.00286, train_auc=0.302, train_tpr=0.000] 
[!] Training of not_augm_mlp_onehot ended:  Thu Jan 18 07:17:53 2024  | Took: 83.79 seconds
[*] Training not_augm_mean_transformer... Started: Thu Jan 18 07:17:53 2024
[!] Scheduler: onecycle | Scheduler step budget: 180
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
C:\Users\dtrizna\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\lightning\pytorch\trainer\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.
  rank_zero_warn(
[*] Training 'not_augm_mean_transformer' model...

   | Name      | Type                   | Params
------------------------------------------------------
0  | model     | MeanTransformerEncoder | 335 K
1  | loss      | BCEWithLogitsLoss      | 0
2  | train_acc | BinaryAccuracy         | 0
3  | train_f1  | BinaryF1Score          | 0
4  | train_auc | BinaryAUROC            | 0
5  | val_acc   | BinaryAccuracy         | 0
6  | val_f1    | BinaryF1Score          | 0
7  | val_auc   | BinaryAUROC            | 0
8  | test_acc  | BinaryAccuracy         | 0
9  | test_f1   | BinaryF1Score          | 0
10 | test_auc  | BinaryAUROC            | 0
------------------------------------------------------
335 K     Trainable params
0         Non-trainable params
335 K     Total params
1.341     Total estimated model params size (MB)
C:\Users\dtrizna\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\lightning\pytorch\trainer\trainer.py:1595: PossibleUserWarning: The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
Epoch 0:  47%|████████████████████████████████████████████████████████████████                                                                        | 8/17 [00:31<00:35,  3.94s/it, loss=0.749, v_num=0, train_loss=0.748, val_acc=0.998, val_f1=0.000, val_auc=0.736, val_tpr=0.000]Metric val_tpr improved. New best score: 0.000
Epoch 0:  94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████        | 16/17 [00:49<00:03,  3.07s/it, loss=0.722, v_num=0, train_loss=0.663, val_acc=0.998, val_f1=0.000, val_auc=0.806, val_tpr=0.270]Metric val_tpr improved by 0.270 >= min_delta = 0.0001. New best score: 0.270
C:\Users\dtrizna\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torchmetrics\utilities\prints.py:42: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
  warnings.warn(*args, **kwargs)  # noqa: B028
C:\Users\dtrizna\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\sklearn\metrics\_ranking.py:1016: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless
  warnings.warn(
Epoch 0: 100%|██████████████████████████████████████████████████████████████████| 17/17 [00:51<00:00,  3.01s/it, loss=0.716, v_num=0, train_loss=0.669, val_acc=0.998, val_f1=0.000, val_auc=0.806, val_tpr=0.270, train_acc=0.575, train_f1=0.00582, train_auc=0.588, train_tpr=nan.0]
NaN or Inf found in input tensor.
Epoch 1:  94%|██████████████████████████████████████████████████████████████    | 16/17 [00:32<00:02,  2.02s/it, loss=0.653, v_num=0, train_loss=0.470, val_acc=0.998, val_f1=0.000, val_auc=0.867, val_tpr=0.270, train_acc=0.575, train_f1=0.00582, train_auc=0.588, train_tpr=nan.0]C:\Users\dtrizna\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torchmetrics\utilities\prints.py:42: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
  warnings.warn(*args, **kwargs)  # noqa: B028
C:\Users\dtrizna\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\sklearn\metrics\_ranking.py:1016: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless
  warnings.warn(
Epoch 1: 100%|███████████████████████████████████████████████████████████████████| 17/17 [00:34<00:00,  2.02s/it, loss=0.642, v_num=0, train_loss=0.460, val_acc=0.998, val_f1=0.000, val_auc=0.867, val_tpr=0.270, train_acc=0.685, train_f1=0.0057, train_auc=0.602, train_tpr=nan.0]
NaN or Inf found in input tensor.
Epoch 2:  94%|███████████████████████████████████████████████████████████████    | 16/17 [00:30<00:01,  1.91s/it, loss=0.491, v_num=0, train_loss=0.249, val_acc=0.998, val_f1=0.000, val_auc=0.866, val_tpr=0.270, train_acc=0.685, train_f1=0.0057, train_auc=0.602, train_tpr=nan.0]C:\Users\dtrizna\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torchmetrics\utilities\prints.py:42: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
  warnings.warn(*args, **kwargs)  # noqa: B028
C:\Users\dtrizna\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\sklearn\metrics\_ranking.py:1016: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless
  warnings.warn(
Epoch 2: 100%|██████████████████████████████████████████████████████████████████| 17/17 [00:32<00:00,  1.91s/it, loss=0.466, v_num=0, train_loss=0.216, val_acc=0.998, val_f1=0.000, val_auc=0.866, val_tpr=0.270, train_acc=0.867, train_f1=0.00843, train_auc=0.626, train_tpr=nan.0]
NaN or Inf found in input tensor.
Epoch 3:  47%|███████████████████████████████▌                                   | 8/17 [00:15<00:17,  1.98s/it, loss=0.368, v_num=0, train_loss=0.152, val_acc=0.998, val_f1=0.000, val_auc=0.865, val_tpr=0.270, train_acc=0.867, train_f1=0.00843, train_auc=0.626, train_tpr=nan.0]Monitored metric val_tpr did not improve in the last 5 records. Best score: 0.270. Signaling Trainer to stop.
Epoch 3:  47%|████████████████████████████████▍                                    | 8/17 [00:15<00:17,  1.99s/it, loss=0.368, v_num=0, train_loss=0.152, val_acc=0.998, val_f1=0.000, val_auc=0.865, val_tpr=0.270, train_acc=0.959, train_f1=0.000, train_auc=0.580, train_tpr=0.000]
Epoch 3:  47%|████████████████████████████████▍                                    | 8/17 [00:15<00:17,  1.99s/it, loss=0.368, v_num=0, train_loss=0.152, val_acc=0.998, val_f1=0.000, val_auc=0.865, val_tpr=0.270, train_acc=0.959, train_f1=0.000, train_auc=0.580, train_tpr=0.000] 
[!] Training of not_augm_mean_transformer ended:  Thu Jan 18 07:20:21 2024  | Took: 148.81 seconds
[*] Training not_augm_cnn... Started: Thu Jan 18 07:20:21 2024
[!] Scheduler: onecycle | Scheduler step budget: 180
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
C:\Users\dtrizna\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\lightning\pytorch\trainer\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.
  rank_zero_warn(
[*] Training 'not_augm_cnn' model...

   | Name      | Type              | Params
-------------------------------------------------
0  | model     | CNN1DGroupedModel | 301 K
1  | loss      | BCEWithLogitsLoss | 0
2  | train_acc | BinaryAccuracy    | 0
3  | train_f1  | BinaryF1Score     | 0
4  | train_auc | BinaryAUROC       | 0
5  | val_acc   | BinaryAccuracy    | 0
6  | val_f1    | BinaryF1Score     | 0
7  | val_auc   | BinaryAUROC       | 0
8  | test_acc  | BinaryAccuracy    | 0
9  | test_f1   | BinaryF1Score     | 0
10 | test_auc  | BinaryAUROC       | 0
-------------------------------------------------
301 K     Trainable params
0         Non-trainable params
301 K     Total params
1.205     Total estimated model params size (MB)
C:\Users\dtrizna\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\lightning\pytorch\trainer\trainer.py:1595: PossibleUserWarning: The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
Epoch 0:  47%|████████████████████████████████████████████████████████████████                                                                        | 8/17 [00:14<00:16,  1.80s/it, loss=0.774, v_num=0, train_loss=0.769, val_acc=0.998, val_f1=0.000, val_auc=0.818, val_tpr=0.000]Metric val_tpr improved. New best score: 0.000
Epoch 0:  94%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏       | 16/17 [00:16<00:01,  1.03s/it, loss=0.765, v_num=0, train_loss=0.749, val_acc=0.462, val_f1=0.00585, val_auc=0.705, val_tpr=0.000]C:\Users\dtrizna\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torchmetrics\utilities\prints.py:42: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
  warnings.warn(*args, **kwargs)  # noqa: B028
C:\Users\dtrizna\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\sklearn\metrics\_ranking.py:1016: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless
  warnings.warn(
Epoch 0: 100%|████████████████████████████████████████████████████████████████| 17/17 [00:16<00:00,  1.02it/s, loss=0.763, v_num=0, train_loss=0.741, val_acc=0.462, val_f1=0.00585, val_auc=0.705, val_tpr=0.000, train_acc=0.241, train_f1=0.00415, train_auc=0.530, train_tpr=nan.0]
NaN or Inf found in input tensor.
Epoch 1:  94%|██████████████████████████████████████████████████████████████    | 16/17 [00:04<00:00,  3.66it/s, loss=0.732, v_num=0, train_loss=0.653, val_acc=0.975, val_f1=0.000, val_auc=0.597, val_tpr=0.000, train_acc=0.241, train_f1=0.00415, train_auc=0.530, train_tpr=nan.0]C:\Users\dtrizna\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torchmetrics\utilities\prints.py:42: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
  warnings.warn(*args, **kwargs)  # noqa: B028
C:\Users\dtrizna\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\sklearn\metrics\_ranking.py:1016: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless
  warnings.warn(
Epoch 1: 100%|██████████████████████████████████████████████████████████████████| 17/17 [00:04<00:00,  3.64it/s, loss=0.727, v_num=0, train_loss=0.636, val_acc=0.975, val_f1=0.000, val_auc=0.597, val_tpr=0.000, train_acc=0.492, train_f1=0.00443, train_auc=0.510, train_tpr=nan.0]
NaN or Inf found in input tensor.
Epoch 2:  94%|██████████████████████████████████████████████████████████████    | 16/17 [00:04<00:00,  3.75it/s, loss=0.636, v_num=0, train_loss=0.438, val_acc=0.998, val_f1=0.000, val_auc=0.493, val_tpr=0.000, train_acc=0.492, train_f1=0.00443, train_auc=0.510, train_tpr=nan.0]Monitored metric val_tpr did not improve in the last 5 records. Best score: 0.000. Signaling Trainer to stop.
Epoch 2:  94%|██████████████████████████████████████████████████████████████    | 16/17 [00:04<00:00,  3.72it/s, loss=0.636, v_num=0, train_loss=0.438, val_acc=0.998, val_f1=0.000, val_auc=0.493, val_tpr=0.000, train_acc=0.884, train_f1=0.0063, train_auc=0.507, train_tpr=0.0417]
Epoch 2:  94%|██████████████████████████████████████████████████████████████    | 16/17 [00:04<00:00,  3.72it/s, loss=0.636, v_num=0, train_loss=0.438, val_acc=0.998, val_f1=0.000, val_auc=0.493, val_tpr=0.000, train_acc=0.884, train_f1=0.0063, train_auc=0.507, train_tpr=0.0417] 
[!] Training of not_augm_cnn ended:  Thu Jan 18 07:21:54 2024  | Took: 92.64 seconds
[!] Script end time: Thu Jan 18 07:21:54 2024
