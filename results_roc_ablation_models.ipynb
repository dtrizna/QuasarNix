{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes of train and test sets: 533014, 470129\n",
      "[*] Loading predictions...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    train_base_parquet_file = [x for x in os.listdir(os.path.join(ROOT,'data/train_baseline.parquet/')) if x.endswith('.parquet')][0]\n",
    "    test_base_parquet_file = [x for x in os.listdir(os.path.join(ROOT,'data/test_baseline.parquet/')) if x.endswith('.parquet')][0]\n",
    "    train_rvrs_parquet_file = [x for x in os.listdir(os.path.join(ROOT,'data/train_rvrs.parquet/')) if x.endswith('.parquet')][0]\n",
    "    test_rvrs_parquet_file = [x for x in os.listdir(os.path.join(ROOT,'data/test_rvrs.parquet/')) if x.endswith('.parquet')][0]\n",
    "\n",
    "    # load as dataframes\n",
    "    train_baseline_df = pd.read_parquet(os.path.join(ROOT,'data/train_baseline.parquet/', train_base_parquet_file))\n",
    "    test_baseline_df = pd.read_parquet(os.path.join(ROOT,'data/test_baseline.parquet/', test_base_parquet_file))\n",
    "    train_malicious_df = pd.read_parquet(os.path.join(ROOT,'data/train_rvrs.parquet/', train_rvrs_parquet_file))\n",
    "    test_malicious_df = pd.read_parquet(os.path.join(ROOT,'data/test_rvrs.parquet/', test_rvrs_parquet_file))\n",
    "\n",
    "    X_train_non_shuffled = train_baseline_df['cmd'].values.tolist() + train_malicious_df['cmd'].values.tolist()\n",
    "    y_train = np.array([0] * len(train_baseline_df) + [1] * len(train_malicious_df), dtype=np.int8)\n",
    "    X_train_cmds, y_train = shuffle(X_train_non_shuffled, y_train, random_state=SEED)\n",
    "\n",
    "    X_test_non_shuffled = test_baseline_df['cmd'].values.tolist() + test_malicious_df['cmd'].values.tolist()\n",
    "    y_test = np.array([0] * len(test_baseline_df) + [1] * len(test_malicious_df), dtype=np.int8)\n",
    "    X_test_cmds, y_test = shuffle(X_test_non_shuffled, y_test, random_state=SEED)\n",
    "\n",
    "    # ===========================================\n",
    "    # DATASET LIMITS FOR TESTING\n",
    "    # ===========================================\n",
    "    X_train_cmds = X_train_cmds[:LIMIT]\n",
    "    y_train = y_train[:LIMIT]\n",
    "    \n",
    "    X_test_cmds = X_test_cmds[:LIMIT]\n",
    "    y_test = y_test[:LIMIT]\n",
    "\n",
    "    return X_train_cmds, y_train, X_test_cmds, y_test\n",
    "\n",
    "SEED = 33\n",
    "\n",
    "VOCAB_SIZE = 4096\n",
    "EMBEDDED_DIM = 64\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 1024\n",
    "DROPOUT = 0.5\n",
    "LIMIT = None\n",
    "DATALOADER_WORKERS = 4\n",
    "\n",
    "\n",
    "# ===========================================\n",
    "# LOADING DATA\n",
    "# ===========================================\n",
    "\n",
    "ROOT = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "X_train_cmds, y_train, X_test_cmds, y_test = load_data()\n",
    "print(f\"Sizes of train and test sets: {len(X_train_cmds)}, {len(X_test_cmds)}\")\n",
    "\n",
    "LOGS_FOLDER = \"logs_models\"\n",
    "y_preds_pickle = os.path.join(LOGS_FOLDER, \"y_preds.pickle\")\n",
    "if os.path.exists(y_preds_pickle):\n",
    "    print(\"[*] Loading predictions...\")\n",
    "    with open(y_preds_pickle, \"rb\") as f:\n",
    "        y_preds = pickle.load(f)\n",
    "else:\n",
    "    y_preds = {}\n",
    "\n",
    "    from watermark import watermark\n",
    "\n",
    "    print(watermark(packages=\"torch,lightning,sklearn\", python=True))\n",
    "    print(f\"[!] Script start time: {time.ctime()}\")\n",
    "\n",
    "    # encoders\n",
    "    from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "    # tokenizers\n",
    "    from nltk.tokenize import wordpunct_tokenize, WhitespaceTokenizer\n",
    "    whitespace_tokenize = WhitespaceTokenizer().tokenize\n",
    "\n",
    "    # modeling\n",
    "    import lightning as L\n",
    "    from lightning.pytorch.loggers import CSVLogger, TensorBoardLogger\n",
    "    from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "    from lightning.lite.utilities.seed import seed_everything\n",
    "\n",
    "    # import random forest, xgboost, and logistic regression\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "    from src.models import *\n",
    "    from src.lit_utils import LitProgressBar\n",
    "    from src.preprocessors import CommandTokenizer, OneHotCustomVectorizer\n",
    "    from src.data_utils import create_dataloader\n",
    "\n",
    "    from typing import List\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def get_tpr_at_fpr(predicted_logits, true_labels, fprNeeded=1e-4):\n",
    "        if isinstance(predicted_logits, torch.Tensor):\n",
    "            predicted_probs = torch.sigmoid(predicted_logits).cpu().detach().numpy()\n",
    "        else:\n",
    "            predicted_probs = sigmoid(predicted_logits)\n",
    "        \n",
    "        if isinstance(true_labels, torch.Tensor):\n",
    "            true_labels = true_labels.cpu().detach().numpy()\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(true_labels, predicted_probs)\n",
    "        if all(np.isnan(fpr)):\n",
    "            return np.nan#, np.nan\n",
    "        else:\n",
    "            tpr_at_fpr = tpr[fpr <= fprNeeded][-1]\n",
    "            #threshold_at_fpr = thresholds[fpr <= fprNeeded][-1]\n",
    "            return tpr_at_fpr#, threshold_at_fpr\n",
    "\n",
    "\n",
    "    def commands_to_loader(cmd: List[str], tokenizer: CommandTokenizer, y: np.ndarray = None) -> DataLoader:\n",
    "        \"\"\"Convert a list of commands to a DataLoader.\"\"\"\n",
    "        tokens = tokenizer.tokenize(cmd)\n",
    "        ints = tokenizer.encode(tokens)\n",
    "        padded = tokenizer.pad(ints, MAX_LEN)\n",
    "        if y is None:\n",
    "            loader = create_dataloader(padded, batch_size=BATCH_SIZE, workers=DATALOADER_WORKERS)\n",
    "        else:\n",
    "            loader = create_dataloader(padded, y, batch_size=BATCH_SIZE, workers=DATALOADER_WORKERS)\n",
    "        return loader\n",
    "\n",
    "\n",
    "    def configure_trainer():\n",
    "        \"\"\"Configure the PyTorch Lightning Trainer.\"\"\"\n",
    "\n",
    "        trainer = L.Trainer(\n",
    "            num_sanity_val_steps=0,\n",
    "            max_epochs=1,\n",
    "            accelerator=\"gpu\",\n",
    "            devices=1,\n",
    "            callbacks=[LitProgressBar()],\n",
    "            logger=TensorBoardLogger(\"logs_temp_results_roc_ablation_models\", name=\"my_model\"),\n",
    "            val_check_interval=0.5,\n",
    "            log_every_n_steps=10,\n",
    "        )\n",
    "        return trainer\n",
    "\n",
    "\n",
    "    def load_lit_model(model_file, pytorch_model, name, log_folder):\n",
    "        lightning_model = PyTorchLightningModel.load_from_checkpoint(checkpoint_path=model_file, model=pytorch_model)\n",
    "        trainer = configure_trainer()\n",
    "        return trainer, lightning_model\n",
    "\n",
    "    seed_everything(SEED)\n",
    "    TOKENIZER = wordpunct_tokenize\n",
    "\n",
    "    # =============================================\n",
    "    # PREPING DATA\n",
    "    # =============================================\n",
    "    tokenizer = CommandTokenizer(tokenizer_fn=TOKENIZER, vocab_size=VOCAB_SIZE)\n",
    "\n",
    "    # ========== EMBEDDING ==========\n",
    "    vocab_file = os.path.join(LOGS_FOLDER, f\"wordpunct_vocab_{VOCAB_SIZE}.json\")\n",
    "    if os.path.exists(vocab_file):\n",
    "        print(\"[*] Loading vocab...\")\n",
    "        tokenizer.load_vocab(vocab_file)\n",
    "    else:\n",
    "        print(\"[*] Building vocab and encoding...\")\n",
    "        X_train_tokens = tokenizer.tokenize(X_train_cmds)\n",
    "        tokenizer.build_vocab(X_train_tokens)\n",
    "        tokenizer.dump_vocab(vocab_file)\n",
    "\n",
    "    # creating dataloaders\n",
    "    # X_train_loader = commands_to_loader(X_train_cmds, tokenizer, y_train)\n",
    "    X_test_loader = commands_to_loader(X_test_cmds, tokenizer, y_test)\n",
    "\n",
    "    # ========== MIN-HASH TABULAR ENCODING ==========\n",
    "    minhash_vectorizer_file = os.path.join(LOGS_FOLDER, f\"minhash_vectorizer_{VOCAB_SIZE}.pkl\")\n",
    "    if os.path.exists(minhash_vectorizer_file):\n",
    "        print(\"[*] Loading MinHash vectorizer...\")\n",
    "        minhash = pickle.load(open(minhash_vectorizer_file, \"rb\"))\n",
    "    else:\n",
    "        minhash = HashingVectorizer(n_features=VOCAB_SIZE, tokenizer=TOKENIZER, token_pattern=None)\n",
    "        print(\"[*] Fitting MinHash encoder...\")\n",
    "        minhash.fit(X_train_cmds)\n",
    "        \n",
    "        with open(minhash_vectorizer_file, \"wb\") as f:\n",
    "            pickle.dump(minhash, f)\n",
    "\n",
    "    # X_train_minhash = minhash.transform(X_train_cmds)\n",
    "    X_test_minhash = minhash.transform(X_test_cmds)\n",
    "\n",
    "    # ========== ONE-HOT TABULAR ENCODING ===========\n",
    "    oh_vectorizer_file = os.path.join(LOGS_FOLDER, f\"onehot_vectorizer_{VOCAB_SIZE}.pkl\")\n",
    "    if os.path.exists(oh_vectorizer_file):\n",
    "        print(\"[*] Loading One-Hot vectorizer...\")\n",
    "        oh = pickle.load(open(oh_vectorizer_file, \"rb\"))\n",
    "    else:\n",
    "        oh = OneHotCustomVectorizer(tokenizer=TOKENIZER, max_features=VOCAB_SIZE)\n",
    "        print(\"[*] Fitting One-Hot encoder...\")\n",
    "        oh.fit(X_train_cmds)\n",
    "\n",
    "        with open(oh_vectorizer_file, \"wb\") as f:\n",
    "            pickle.dump(oh, f)\n",
    "\n",
    "    # X_train_onehot = oh.transform(X_train_cmds)\n",
    "    X_test_onehot = oh.transform(X_test_cmds)\n",
    "\n",
    "    # =============================================\n",
    "    # DEFINING MODELS\n",
    "    # =============================================\n",
    "    print(f\"[*] Defining models...\")\n",
    "\n",
    "    # sequence models\n",
    "    mlp_seq_model = SimpleMLPWithEmbedding(vocab_size=VOCAB_SIZE, embedding_dim=EMBEDDED_DIM, output_dim=1, hidden_dim=[256, 64, 32], use_positional_encoding=False, max_len=MAX_LEN, dropout=DROPOUT) # 297 K params\n",
    "    cnn_model = CNN1DGroupedModel(vocab_size=VOCAB_SIZE, embed_dim=EMBEDDED_DIM, num_channels=32, kernel_sizes=[2, 3, 4, 5], mlp_hidden_dims=[64, 32], output_dim=1, dropout=DROPOUT) # 301 K params\n",
    "    lstm_model = BiLSTMModel(vocab_size=VOCAB_SIZE, embed_dim=EMBEDDED_DIM, hidden_dim=32, mlp_hidden_dims=[64, 32], output_dim=1, dropout=DROPOUT) # 318 K params\n",
    "    cnn_lstm_model = CNN1D_BiLSTM_Model(vocab_size=VOCAB_SIZE, embed_dim=EMBEDDED_DIM, num_channels=32, kernel_size=3, lstm_hidden_dim=32, mlp_hidden_dims=[64, 32], output_dim=1, dropout=DROPOUT) # 316 K params\n",
    "    mean_transformer_model = MeanTransformerEncoder(vocab_size=VOCAB_SIZE, d_model=EMBEDDED_DIM, nhead=4, num_layers=2, dim_feedforward=128, max_len=MAX_LEN, dropout=DROPOUT, mlp_hidden_dims=[64,32], output_dim=1) # 335 K params\n",
    "    cls_transformer_model = CLSTransformerEncoder(vocab_size=VOCAB_SIZE, d_model=EMBEDDED_DIM, nhead=4, num_layers=2, dim_feedforward=128, max_len=MAX_LEN, dropout=DROPOUT, mlp_hidden_dims=[64,32], output_dim=1) #  335 K params\n",
    "    attpool_transformer_model = AttentionPoolingTransformerEncoder(vocab_size=VOCAB_SIZE, d_model=EMBEDDED_DIM, nhead=4, num_layers=2, dim_feedforward=128, max_len=MAX_LEN, dropout=DROPOUT, mlp_hidden_dims=[64,32], output_dim=1) #  335 K params\n",
    "    neurlux = NeurLuxModel(vocab_size=VOCAB_SIZE, embed_dim=EMBEDDED_DIM, max_len=MAX_LEN, hidden_dim=32, output_dim=1, dropout=DROPOUT) # 402 K params\n",
    "\n",
    "    # tabular models\n",
    "    rf_model_minhash = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=SEED)\n",
    "    xgb_model_minhash = XGBClassifier(n_estimators=100, max_depth=10, random_state=SEED)\n",
    "    log_reg_minhash = LogisticRegression(random_state=SEED)\n",
    "    mlp_tab_model_minhash = SimpleMLP(input_dim=VOCAB_SIZE, output_dim=1, hidden_dim=[64, 32], dropout=DROPOUT) # 264 K params\n",
    "    rf_model_onehot = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=SEED)\n",
    "    xgb_model_onehot = XGBClassifier(n_estimators=100, max_depth=10, random_state=SEED)\n",
    "    log_reg_onehot = LogisticRegression(random_state=SEED)\n",
    "    mlp_tab_model_onehot = SimpleMLP(input_dim=VOCAB_SIZE, output_dim=1, hidden_dim=[64, 32], dropout=DROPOUT) # 264 K params\n",
    "\n",
    "    models = {\n",
    "        \"_tabular_mlp_minhash\": mlp_tab_model_minhash,\n",
    "        \"_tabular_rf_minhash\": rf_model_minhash,\n",
    "        \"_tabular_xgb_minhash\": xgb_model_minhash,\n",
    "        \"_tabular_log_reg_minhash\": log_reg_minhash,\n",
    "        \"_tabular_mlp_onehot\": mlp_tab_model_onehot,\n",
    "        \"_tabular_rf_onehot\": rf_model_onehot,\n",
    "        \"_tabular_xgb_onehot\": xgb_model_onehot,\n",
    "        \"_tabular_log_reg_onehot\": log_reg_onehot,\n",
    "        \"mlp_seq\": mlp_seq_model,\n",
    "        \"attpool_transformer\": attpool_transformer_model,\n",
    "        \"cls_transformer\": cls_transformer_model,\n",
    "        \"mean_transformer\": mean_transformer_model,\n",
    "        \"neurlux\": neurlux,\n",
    "        \"cnn\": cnn_model,\n",
    "        \"lstm\": lstm_model,\n",
    "        \"cnn_lstm\": cnn_lstm_model,\n",
    "    }\n",
    "\n",
    "    for name, model in models.items():\n",
    "        if name in y_preds:\n",
    "            print(f\"[*] Model {name} already predicted\")\n",
    "            continue\n",
    "\n",
    "        if name.startswith(\"_tabular\") and \"mlp\" not in name:\n",
    "            \n",
    "            model_file = os.path.join(LOGS_FOLDER, name, \"model.pkl\")\n",
    "            print(f\"[*] Loading {name} from {model_file}...\")\n",
    "            with open(model_file, \"rb\") as f:\n",
    "                model = pickle.load(f)\n",
    "            \n",
    "            preprocessor = name.split(\"_\")[-1]\n",
    "            assert preprocessor in [\"onehot\", \"minhash\"]\n",
    "\n",
    "            if preprocessor == \"onehot\":\n",
    "                # x_train = X_train_onehot\n",
    "                x_test = X_test_onehot\n",
    "            \n",
    "            elif preprocessor == \"minhash\":\n",
    "                # x_train = X_train_minhash\n",
    "                x_test = X_test_minhash\n",
    "\n",
    "            print(f\"[*] Predicting with {name}...\")\n",
    "            y_test_preds = model.predict_proba(x_test)[:,1]\n",
    "            y_preds[name] = y_test_preds\n",
    "        \n",
    "        else:    \n",
    "            if \"tabular\" in name:\n",
    "                preprocessor = name.split(\"_\")[-1]\n",
    "                assert preprocessor in [\"onehot\", \"minhash\"]\n",
    "\n",
    "                if preprocessor == \"onehot\":\n",
    "                    # x_train = X_train_onehot\n",
    "                    x_test = X_test_onehot\n",
    "                \n",
    "                elif preprocessor == \"minhash\":\n",
    "                    # x_train = X_train_minhash\n",
    "                    x_test = X_test_minhash\n",
    "\n",
    "                # train_loader = create_dataloader(x_train, y_train, batch_size=BATCH_SIZE, workers=DATALOADER_WORKERS)\n",
    "                test_loader = create_dataloader(x_test, y_test, batch_size=BATCH_SIZE, workers=DATALOADER_WORKERS)\n",
    "            \n",
    "            else:\n",
    "                # train_loader = X_train_loader\n",
    "                test_loader = X_test_loader\n",
    "            \n",
    "            chkp_folder = os.path.join(LOGS_FOLDER, f\"{name}_csv\", \"version_0\", \"checkpoints\")\n",
    "            if not os.path.exists(chkp_folder):\n",
    "                print(f\"Model {name} not trained yet\")\n",
    "\n",
    "            best_model = [x for x in os.listdir(os.path.join(LOGS_FOLDER, f\"{name}_csv\", \"version_0\", \"checkpoints\")) if x.startswith(\"epoch\")][0]\n",
    "            best_model = os.path.join(chkp_folder, best_model)\n",
    "            print(\"Best model: \", best_model)\n",
    "            trainer, lightning_model = load_lit_model(best_model, model, name, LOGS_FOLDER)\n",
    "            print(f\"[*] Predicting with {name}...\")\n",
    "            y_pred_proba = trainer.predict(lightning_model, test_loader, return_predictions=True)\n",
    "            if isinstance(y_pred_proba, list):\n",
    "                y_pred_proba = np.vstack(y_pred_proba).squeeze()\n",
    "\n",
    "            y_preds[name] = y_pred_proba\n",
    "\n",
    "    with open(y_preds_pickle, \"wb\") as f:\n",
    "        pickle.dump(y_preds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x311.489 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scienceplots # used by plt.style\n",
    "\n",
    "from src.plots import set_size, plot_roc_curve\n",
    "\n",
    "import scienceplots\n",
    "plt.style.use(['science', 'no-latex'])\n",
    "plt.rc('text', usetex=True)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=set_size())\n",
    "\n",
    "to_skip = [x for x in y_preds.keys() if 'tabular' in x]\n",
    "to_keep = {\n",
    "    # \"_tabular_log_reg_onehot\": 'LogR (One-Hot)',\n",
    "    'cnn': '1D-CNN + MLP',\n",
    "    '_tabular_mlp_onehot': 'MLP (One-Hot)',\n",
    "    '_tabular_xgb_onehot': 'GBDT (One-Hot)',\n",
    "    'mlp_seq': 'MLP (Embedding)',\n",
    "    # 'neurlux': '1D-CNN + LSTM\\n+ Attention + MLP',\n",
    "    # 'cls_transformer': 'Transformer (CLS)',\n",
    "    'mean_transformer': 'Transformer\\n(Mean Pooling)',\n",
    "    #'_tabular_mlp_minhash': 'MLP (Minhash)',\n",
    "    # 'lstm': 'LSTM + MLP',\n",
    "    # 'cnn_lstm': '1D-CNN + LSTM\\n+ MLP',\n",
    "    #'attpool_transformer': 'Transformer\\n(Attent. Pooling)',\n",
    "}\n",
    "for name in to_keep.keys():\n",
    "    y_pred_proba = y_preds[name]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "    name = to_keep[name]\n",
    "    plot_roc_curve(fpr, tpr, None, model_name=name, ax=ax, semilogx=True, xlim=[2e-6,1e-2], ylim=[0.6,1.05])\n",
    "\n",
    "# Create a mapping from labels to handles\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "label_to_handle = {label: handle for handle, label in zip(handles, labels)}\n",
    "\n",
    "# labels for ROC curve\n",
    "ax.set_xlabel(\"False Positive Rate\", fontsize=20)\n",
    "#ax.set_ylabel(\"True Positive Rate\", fontsize=18)\n",
    "ax.set_title(\"Test set ROC curves\", fontsize=18)\n",
    "\n",
    "# ticklbel fontsizes to 18 as well\n",
    "ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "# Order the handles based on the order of labels in to_keep.values()\n",
    "new_handles = [label_to_handle[label] for label in to_keep.values()]\n",
    "new_labels = list(to_keep.values())\n",
    "ax.grid(linewidth=0.2)\n",
    "_ = ax.legend(new_handles, new_labels, ncol=1, fontsize=16, bbox_to_anchor=(1.03, 0), loc='lower right')\n",
    "\n",
    "# save as pdf in \"/img\"\n",
    "fig.savefig(f\"img/roc_ablation_models.pdf\", bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>TPR at FPR=10^-4</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_tabular_mlp_minhash</td>\n",
       "      <td>0.982039</td>\n",
       "      <td>0.985962</td>\n",
       "      <td>0.999899</td>\n",
       "      <td>0.972407</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.986155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_tabular_rf_minhash</td>\n",
       "      <td>0.790049</td>\n",
       "      <td>0.666658</td>\n",
       "      <td>0.49999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999948</td>\n",
       "      <td>0.49999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_tabular_xgb_minhash</td>\n",
       "      <td>0.963397</td>\n",
       "      <td>0.66666</td>\n",
       "      <td>0.499993</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999944</td>\n",
       "      <td>0.499995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_tabular_log_reg_minhash</td>\n",
       "      <td>0.778231</td>\n",
       "      <td>0.666658</td>\n",
       "      <td>0.49999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999818</td>\n",
       "      <td>0.49999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_tabular_mlp_onehot</td>\n",
       "      <td>0.997205</td>\n",
       "      <td>0.997621</td>\n",
       "      <td>0.999923</td>\n",
       "      <td>0.995329</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.997626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>_tabular_rf_onehot</td>\n",
       "      <td>0.952684</td>\n",
       "      <td>0.666658</td>\n",
       "      <td>0.49999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999982</td>\n",
       "      <td>0.49999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>_tabular_xgb_onehot</td>\n",
       "      <td>0.999426</td>\n",
       "      <td>0.666666</td>\n",
       "      <td>0.499999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>_tabular_log_reg_onehot</td>\n",
       "      <td>0.998843</td>\n",
       "      <td>0.666658</td>\n",
       "      <td>0.49999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.49999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mlp_seq</td>\n",
       "      <td>0.739207</td>\n",
       "      <td>0.951933</td>\n",
       "      <td>0.999883</td>\n",
       "      <td>0.908372</td>\n",
       "      <td>0.999958</td>\n",
       "      <td>0.954134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>attpool_transformer</td>\n",
       "      <td>0.910929</td>\n",
       "      <td>0.976357</td>\n",
       "      <td>0.999857</td>\n",
       "      <td>0.953935</td>\n",
       "      <td>0.999875</td>\n",
       "      <td>0.9769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cls_transformer</td>\n",
       "      <td>0.991555</td>\n",
       "      <td>0.996874</td>\n",
       "      <td>0.999872</td>\n",
       "      <td>0.993895</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.996884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mean_transformer</td>\n",
       "      <td>0.952314</td>\n",
       "      <td>0.988769</td>\n",
       "      <td>0.999878</td>\n",
       "      <td>0.977904</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.988892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>neurlux</td>\n",
       "      <td>0.9089</td>\n",
       "      <td>0.902162</td>\n",
       "      <td>0.99999</td>\n",
       "      <td>0.821769</td>\n",
       "      <td>0.999957</td>\n",
       "      <td>0.910882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cnn</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974025</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.949366</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>lstm</td>\n",
       "      <td>0.991019</td>\n",
       "      <td>0.950764</td>\n",
       "      <td>0.999986</td>\n",
       "      <td>0.90616</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.953075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cnn_lstm</td>\n",
       "      <td>0.869433</td>\n",
       "      <td>0.815259</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.688139</td>\n",
       "      <td>0.995315</td>\n",
       "      <td>0.844068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model TPR at FPR=10^-4  F1-Score Precision    Recall  \\\n",
       "0       _tabular_mlp_minhash         0.982039  0.985962  0.999899  0.972407   \n",
       "1        _tabular_rf_minhash         0.790049  0.666658   0.49999       1.0   \n",
       "2       _tabular_xgb_minhash         0.963397   0.66666  0.499993       1.0   \n",
       "3   _tabular_log_reg_minhash         0.778231  0.666658   0.49999       1.0   \n",
       "4        _tabular_mlp_onehot         0.997205  0.997621  0.999923  0.995329   \n",
       "5         _tabular_rf_onehot         0.952684  0.666658   0.49999       1.0   \n",
       "6        _tabular_xgb_onehot         0.999426  0.666666  0.499999       1.0   \n",
       "7    _tabular_log_reg_onehot         0.998843  0.666658   0.49999       1.0   \n",
       "8                    mlp_seq         0.739207  0.951933  0.999883  0.908372   \n",
       "9        attpool_transformer         0.910929  0.976357  0.999857  0.953935   \n",
       "10           cls_transformer         0.991555  0.996874  0.999872  0.993895   \n",
       "11          mean_transformer         0.952314  0.988769  0.999878  0.977904   \n",
       "12                   neurlux           0.9089  0.902162   0.99999  0.821769   \n",
       "13                       cnn              1.0  0.974025       1.0  0.949366   \n",
       "14                      lstm         0.991019  0.950764  0.999986   0.90616   \n",
       "15                  cnn_lstm         0.869433  0.815259  0.999988  0.688139   \n",
       "\n",
       "         AUC  Accuracy  \n",
       "0   0.999989  0.986155  \n",
       "1   0.999948   0.49999  \n",
       "2   0.999944  0.499995  \n",
       "3   0.999818   0.49999  \n",
       "4   0.999999  0.997626  \n",
       "5   0.999982   0.49999  \n",
       "6        1.0  0.500007  \n",
       "7        1.0   0.49999  \n",
       "8   0.999958  0.954134  \n",
       "9   0.999875    0.9769  \n",
       "10  0.999996  0.996884  \n",
       "11  0.999988  0.988892  \n",
       "12  0.999957  0.910882  \n",
       "13       1.0  0.974684  \n",
       "14  0.999983  0.953075  \n",
       "15  0.995315  0.844068  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "\n",
    "def get_tpr_at_fpr(predicted_logits, true_labels, fprNeeded=1e-4):\n",
    "    predicted_probs = torch.sigmoid(predicted_logits).cpu().detach().numpy()\n",
    "    fpr, tpr, _ = roc_curve(true_labels, predicted_probs)\n",
    "    if all(np.isnan(fpr)):\n",
    "        return np.nan#, np.nan\n",
    "    else:\n",
    "        tpr_at_fpr = tpr[fpr <= fprNeeded][-1]\n",
    "        return tpr_at_fpr\n",
    "\n",
    "results_df = pd.DataFrame(columns=['Model', 'TPR at FPR=10^-4', 'F1-Score', 'Precision', 'Recall', 'AUC', 'Accuracy'])\n",
    "\n",
    "\n",
    "# Loop through each key-value pair in your dictionary, compute the required metrics, and append them to results_df\n",
    "for model_name, predictions in y_preds.items():\n",
    "    predicted_probs = torch.sigmoid(torch.tensor(predictions)).cpu().detach().numpy()\n",
    "    binary_preds = (predicted_probs > 0.5).astype(int)  # Convert probabilities to binary labels with a 0.5 threshold\n",
    "    \n",
    "    auc = roc_auc_score(y_test, predicted_probs)\n",
    "    f1 = f1_score(y_test, binary_preds)\n",
    "    precision = precision_score(y_test, binary_preds)\n",
    "    recall = recall_score(y_test, binary_preds)\n",
    "    accuracy = accuracy_score(y_test, binary_preds)\n",
    "    tpr_at_fpr_10e4 = get_tpr_at_fpr(torch.tensor(predictions), y_test)\n",
    "    \n",
    "    # Append results to dataframe\n",
    "    new_row = pd.DataFrame({\n",
    "        'Model': [model_name],\n",
    "        'TPR at FPR=10^-4': [tpr_at_fpr_10e4],\n",
    "        'F1-Score': [f1],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall],\n",
    "        'Accuracy': [accuracy],\n",
    "        'AUC': [auc]\n",
    "    })\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "# Output the results\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scienceplots\n",
    "plt.style.use(['science', 'no-latex'])\n",
    "plt.rc('text', usetex=True)\n",
    "\n",
    "from src.plots import set_size\n",
    "\n",
    "df_dict = {}\n",
    "LOGDIR = 'logs_models'\n",
    "\n",
    "# key_extractor = lambda x: x.split('_')[1:]\n",
    "\n",
    "for subfolder in os.listdir(LOGDIR):\n",
    "    subfolder_path = os.path.join(LOGDIR, subfolder)\n",
    "    csv_file_path = os.path.join(subfolder_path, 'version_0', 'metrics.csv')\n",
    "    if os.path.exists(csv_file_path):\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        encoder = ' '.join(subfolder.split('_')[:-1]).title()\n",
    "        df_dict[encoder] = df\n",
    "\n",
    "\n",
    "def extract_metric_values(df_dict, metric_name, operation='last'):\n",
    "    \"\"\"\n",
    "    Extracts specified metric values (either last or max) from nested DataFrames in df_dict.\n",
    "    \n",
    "    Parameters:\n",
    "        df_dict (defaultdict(dict)): Nested dictionary of DataFrames keyed by tokenizer and vocab_size.\n",
    "        metric_name (str): The name of the metric to extract.\n",
    "        operation (str): The operation to perform ('last' or 'max').\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame containing the extracted metric values, indexed by vocab_size and columns by tokenizer.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to temporarily store the extracted values\n",
    "    extracted_values_temp = {}\n",
    "    \n",
    "    # Loop through the nested DataFrames\n",
    "    for encoder, df in df_dict.items():\n",
    "        if metric_name in df.columns:\n",
    "            if operation == 'last':\n",
    "                # Extract the last value of the specified metric\n",
    "                last_value = df[metric_name].dropna().iloc[-1]\n",
    "                extracted_values_temp[encoder] = last_value\n",
    "                \n",
    "            elif operation == 'max':\n",
    "                # Extract the maximum value of the specified metric\n",
    "                max_value = df[metric_name].dropna().max()\n",
    "                extracted_values_temp[encoder] = max_value\n",
    "            else:\n",
    "                print(f\"Invalid operation '{operation}' specified. Skipping {encoder}.\")\n",
    "        else:\n",
    "            print(f\"Metric '{metric_name}' not found in DataFrame for {encoder}. Skipping.\")\n",
    "                \n",
    "    # Convert the nested dictionary to a DataFrame, using 'metric_name' as value for index\n",
    "    extracted_values_df = pd.DataFrame.from_dict(extracted_values_temp, orient='index', columns=[metric_name]).transpose()\n",
    "\n",
    "    # sort index by vocab_size\n",
    "    extracted_values_df.sort_index(inplace=True)\n",
    "\n",
    "    return extracted_values_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_tpr</th>\n",
       "      <th>val_f1</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Attpool Transformer</th>\n",
       "      <td>0.967858</td>\n",
       "      <td>0.982970</td>\n",
       "      <td>0.983254</td>\n",
       "      <td>0.999406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cls Transformer</th>\n",
       "      <td>0.997018</td>\n",
       "      <td>0.995994</td>\n",
       "      <td>0.996010</td>\n",
       "      <td>0.999996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cnn</th>\n",
       "      <td>0.999910</td>\n",
       "      <td>0.974444</td>\n",
       "      <td>0.975081</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cnn Lstm</th>\n",
       "      <td>0.780494</td>\n",
       "      <td>0.642088</td>\n",
       "      <td>0.736428</td>\n",
       "      <td>0.996596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lstm</th>\n",
       "      <td>0.972996</td>\n",
       "      <td>0.940016</td>\n",
       "      <td>0.943411</td>\n",
       "      <td>0.999938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean Transformer</th>\n",
       "      <td>0.967348</td>\n",
       "      <td>0.994659</td>\n",
       "      <td>0.994687</td>\n",
       "      <td>0.999963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mlp Seq</th>\n",
       "      <td>0.969886</td>\n",
       "      <td>0.917868</td>\n",
       "      <td>0.924095</td>\n",
       "      <td>0.999945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neurlux</th>\n",
       "      <td>0.968056</td>\n",
       "      <td>0.900832</td>\n",
       "      <td>0.909776</td>\n",
       "      <td>0.999922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tabular Mlp Minhash</th>\n",
       "      <td>0.993898</td>\n",
       "      <td>0.986445</td>\n",
       "      <td>0.986625</td>\n",
       "      <td>0.999988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tabular Mlp Onehot</th>\n",
       "      <td>0.999303</td>\n",
       "      <td>0.993978</td>\n",
       "      <td>0.994014</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Log Reg Minhash</th>\n",
       "      <td>0.778231</td>\n",
       "      <td>0.925402</td>\n",
       "      <td>0.930557</td>\n",
       "      <td>0.999818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Log Reg Onehot</th>\n",
       "      <td>0.998843</td>\n",
       "      <td>0.999268</td>\n",
       "      <td>0.999268</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rf Minhash</th>\n",
       "      <td>0.790049</td>\n",
       "      <td>0.995738</td>\n",
       "      <td>0.995720</td>\n",
       "      <td>0.999948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rf Onehot</th>\n",
       "      <td>0.952684</td>\n",
       "      <td>0.993455</td>\n",
       "      <td>0.993412</td>\n",
       "      <td>0.999982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Xgb Minhash</th>\n",
       "      <td>0.963397</td>\n",
       "      <td>0.999381</td>\n",
       "      <td>0.999381</td>\n",
       "      <td>0.999944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Xgb Onehot</th>\n",
       "      <td>0.999426</td>\n",
       "      <td>0.999657</td>\n",
       "      <td>0.999658</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       val_tpr    val_f1   val_acc   val_auc\n",
       "Attpool Transformer   0.967858  0.982970  0.983254  0.999406\n",
       "Cls Transformer       0.997018  0.995994  0.996010  0.999996\n",
       "Cnn                   0.999910  0.974444  0.975081  1.000000\n",
       "Cnn Lstm              0.780494  0.642088  0.736428  0.996596\n",
       "Lstm                  0.972996  0.940016  0.943411  0.999938\n",
       "Mean Transformer      0.967348  0.994659  0.994687  0.999963\n",
       "Mlp Seq               0.969886  0.917868  0.924095  0.999945\n",
       "Neurlux               0.968056  0.900832  0.909776  0.999922\n",
       " Tabular Mlp Minhash  0.993898  0.986445  0.986625  0.999988\n",
       " Tabular Mlp Onehot   0.999303  0.993978  0.994014  0.999999\n",
       "Log Reg Minhash       0.778231  0.925402  0.930557  0.999818\n",
       "Log Reg Onehot        0.998843  0.999268  0.999268  1.000000\n",
       "Rf Minhash            0.790049  0.995738  0.995720  0.999948\n",
       "Rf Onehot             0.952684  0.993455  0.993412  0.999982\n",
       "Xgb Minhash           0.963397  0.999381  0.999381  0.999944\n",
       "Xgb Onehot            0.999426  0.999657  0.999658  1.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = ['val_tpr', 'val_f1', 'val_acc', 'val_auc']\n",
    "df = pd.DataFrame()\n",
    "for metric in metrics:\n",
    "    df1 = extract_metric_values(df_dict, metric, operation='last')\n",
    "    df = pd.concat([df, df1], axis=0)\n",
    "\n",
    "tabular_df_dict = {}\n",
    "for subfolder in os.listdir(LOGDIR):\n",
    "    subfolder_path = os.path.join(LOGDIR, subfolder)\n",
    "    csv_file_path = os.path.join(subfolder_path, 'metrics.csv')\n",
    "    if os.path.exists(csv_file_path):\n",
    "        ldf = pd.read_csv(csv_file_path)\n",
    "        encoder = ' '.join(subfolder.split('_')[2:]).title()\n",
    "        tabular_df_dict[encoder] = ldf\n",
    "\n",
    "col_map = dict(zip(['tpr', 'f1', 'acc', 'auc'], ['val_tpr', 'val_f1', 'val_acc', 'val_auc']))\n",
    "\n",
    "# Iterate over each item in the dictionary, map the columns, and concatenate to the initial dataframe\n",
    "for model_name, ldf in tabular_df_dict.items():\n",
    "    # Map columns\n",
    "    ldf = ldf.rename(columns=col_map)\n",
    "    # Transpose the dataframe so that metrics are in the columns and model names are in the index\n",
    "    ldf = ldf.T\n",
    "    # Rename the columns of df to match the model name\n",
    "    ldf.columns = [model_name]\n",
    "    # Concatenate along columns axis\n",
    "    df = pd.concat([df, ldf], axis=1)\n",
    "\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_tpr</th>\n",
       "      <th>val_f1</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Attpool Transformer</th>\n",
       "      <td>0.983769</td>\n",
       "      <td>0.999551</td>\n",
       "      <td>0.999551</td>\n",
       "      <td>0.999963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cls Transformer</th>\n",
       "      <td>0.997021</td>\n",
       "      <td>0.999889</td>\n",
       "      <td>0.999889</td>\n",
       "      <td>0.999996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cnn</th>\n",
       "      <td>0.999910</td>\n",
       "      <td>0.999949</td>\n",
       "      <td>0.999949</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cnn Lstm</th>\n",
       "      <td>0.916991</td>\n",
       "      <td>0.986644</td>\n",
       "      <td>0.986585</td>\n",
       "      <td>0.997550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lstm</th>\n",
       "      <td>0.992145</td>\n",
       "      <td>0.967622</td>\n",
       "      <td>0.968636</td>\n",
       "      <td>0.999983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean Transformer</th>\n",
       "      <td>0.992789</td>\n",
       "      <td>0.999902</td>\n",
       "      <td>0.999902</td>\n",
       "      <td>0.999988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mlp Seq</th>\n",
       "      <td>0.980032</td>\n",
       "      <td>0.990541</td>\n",
       "      <td>0.990622</td>\n",
       "      <td>0.999957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neurlux</th>\n",
       "      <td>0.980555</td>\n",
       "      <td>0.985019</td>\n",
       "      <td>0.984945</td>\n",
       "      <td>0.999957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tabular Mlp Minhash</th>\n",
       "      <td>0.994517</td>\n",
       "      <td>0.999298</td>\n",
       "      <td>0.999298</td>\n",
       "      <td>0.999989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tabular Mlp Onehot</th>\n",
       "      <td>0.999554</td>\n",
       "      <td>0.999777</td>\n",
       "      <td>0.999777</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Log Reg Minhash</th>\n",
       "      <td>0.778231</td>\n",
       "      <td>0.925402</td>\n",
       "      <td>0.930557</td>\n",
       "      <td>0.999818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Log Reg Onehot</th>\n",
       "      <td>0.998843</td>\n",
       "      <td>0.999268</td>\n",
       "      <td>0.999268</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rf Minhash</th>\n",
       "      <td>0.790049</td>\n",
       "      <td>0.995738</td>\n",
       "      <td>0.995720</td>\n",
       "      <td>0.999948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rf Onehot</th>\n",
       "      <td>0.952684</td>\n",
       "      <td>0.993455</td>\n",
       "      <td>0.993412</td>\n",
       "      <td>0.999982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Xgb Minhash</th>\n",
       "      <td>0.963397</td>\n",
       "      <td>0.999381</td>\n",
       "      <td>0.999381</td>\n",
       "      <td>0.999944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Xgb Onehot</th>\n",
       "      <td>0.999426</td>\n",
       "      <td>0.999657</td>\n",
       "      <td>0.999658</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       val_tpr    val_f1   val_acc   val_auc\n",
       "Attpool Transformer   0.983769  0.999551  0.999551  0.999963\n",
       "Cls Transformer       0.997021  0.999889  0.999889  0.999996\n",
       "Cnn                   0.999910  0.999949  0.999949  1.000000\n",
       "Cnn Lstm              0.916991  0.986644  0.986585  0.997550\n",
       "Lstm                  0.992145  0.967622  0.968636  0.999983\n",
       "Mean Transformer      0.992789  0.999902  0.999902  0.999988\n",
       "Mlp Seq               0.980032  0.990541  0.990622  0.999957\n",
       "Neurlux               0.980555  0.985019  0.984945  0.999957\n",
       " Tabular Mlp Minhash  0.994517  0.999298  0.999298  0.999989\n",
       " Tabular Mlp Onehot   0.999554  0.999777  0.999777  0.999999\n",
       "Log Reg Minhash       0.778231  0.925402  0.930557  0.999818\n",
       "Log Reg Onehot        0.998843  0.999268  0.999268  1.000000\n",
       "Rf Minhash            0.790049  0.995738  0.995720  0.999948\n",
       "Rf Onehot             0.952684  0.993455  0.993412  0.999982\n",
       "Xgb Minhash           0.963397  0.999381  0.999381  0.999944\n",
       "Xgb Onehot            0.999426  0.999657  0.999658  1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = ['val_tpr', 'val_f1', 'val_acc', 'val_auc']\n",
    "df = pd.DataFrame()\n",
    "for metric in metrics:\n",
    "    df1 = extract_metric_values(df_dict, metric, operation='max')\n",
    "    df = pd.concat([df, df1], axis=0)\n",
    "\n",
    "tabular_df_dict = {}\n",
    "for subfolder in os.listdir(LOGDIR):\n",
    "    subfolder_path = os.path.join(LOGDIR, subfolder)\n",
    "    csv_file_path = os.path.join(subfolder_path, 'metrics.csv')\n",
    "    if os.path.exists(csv_file_path):\n",
    "        ldf = pd.read_csv(csv_file_path)\n",
    "        encoder = ' '.join(subfolder.split('_')[2:]).title()\n",
    "        tabular_df_dict[encoder] = ldf\n",
    "\n",
    "col_map = dict(zip(['tpr', 'f1', 'acc', 'auc'], ['val_tpr', 'val_f1', 'val_acc', 'val_auc']))\n",
    "\n",
    "# Iterate over each item in the dictionary, map the columns, and concatenate to the initial dataframe\n",
    "for model_name, ldf in tabular_df_dict.items():\n",
    "    # Map columns\n",
    "    ldf = ldf.rename(columns=col_map)\n",
    "    # Transpose the dataframe so that metrics are in the columns and model names are in the index\n",
    "    ldf = ldf.T\n",
    "    # Rename the columns of df to match the model name\n",
    "    ldf.columns = [model_name]\n",
    "    # Concatenate along columns axis\n",
    "    df = pd.concat([df, ldf], axis=1)\n",
    "\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
