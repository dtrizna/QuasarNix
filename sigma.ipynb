{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_cmds:  501573\n",
      "X_test_cmds:  501570\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, roc_auc_score, roc_curve, auc\n",
    "\n",
    "LIMIT = None\n",
    "SEED = 33\n",
    "ROOT = os.path.dirname(os.path.abspath('__file__'))\n",
    "sigma_yml = os.path.join(ROOT, \"data\", \"rvrs_sigma.yml\")\n",
    "\n",
    "def load_data():\n",
    "    train_base_parquet_file = [x for x in os.listdir(os.path.join(ROOT,'data/train_baseline.parquet/')) if x.endswith('.parquet')][0]\n",
    "    test_base_parquet_file = [x for x in os.listdir(os.path.join(ROOT,'data/test_baseline.parquet/')) if x.endswith('.parquet')][0]\n",
    "    train_rvrs_parquet_file = [x for x in os.listdir(os.path.join(ROOT,'data/train_rvrs.parquet/')) if x.endswith('.parquet')][0]\n",
    "    test_rvrs_parquet_file = [x for x in os.listdir(os.path.join(ROOT,'data/test_rvrs.parquet/')) if x.endswith('.parquet')][0]\n",
    "\n",
    "    # load as dataframes\n",
    "    train_baseline_df = pd.read_parquet(os.path.join(ROOT,'data/train_baseline.parquet/', train_base_parquet_file))\n",
    "    test_baseline_df = pd.read_parquet(os.path.join(ROOT,'data/test_baseline.parquet/', test_base_parquet_file))\n",
    "    train_malicious_df = pd.read_parquet(os.path.join(ROOT,'data/train_rvrs.parquet/', train_rvrs_parquet_file))\n",
    "    test_malicious_df = pd.read_parquet(os.path.join(ROOT,'data/test_rvrs.parquet/', test_rvrs_parquet_file))\n",
    "\n",
    "    # X_train_non_shuffled = train_baseline_df['cmd'].values.tolist() + train_malicious_df['cmd'].values.tolist()\n",
    "    X_train_non_shuffled = train_baseline_df['cmd'].values.tolist() + test_malicious_df['cmd'].values.tolist()\n",
    "    # y_train = np.array([0] * len(train_baseline_df) + [1] * len(train_malicious_df), dtype=np.int8)\n",
    "    y_train = np.array([0] * len(train_baseline_df) + [1] * len(test_malicious_df), dtype=np.int8)\n",
    "    X_train_cmds, y_train = shuffle(X_train_non_shuffled, y_train, random_state=SEED)\n",
    "\n",
    "    # X_test_non_shuffled = test_baseline_df['cmd'].values.tolist() + test_malicious_df['cmd'].values.tolist()\n",
    "    X_test_non_shuffled = test_baseline_df['cmd'].values.tolist() + train_malicious_df['cmd'].values.tolist()\n",
    "    # y_test = np.array([0] * len(test_baseline_df) + [1] * len(test_malicious_df), dtype=np.int8)\n",
    "    y_test = np.array([0] * len(test_baseline_df) + [1] * len(train_malicious_df), dtype=np.int8)\n",
    "    X_test_cmds, y_test = shuffle(X_test_non_shuffled, y_test, random_state=SEED)\n",
    "\n",
    "    # ===========================================\n",
    "    # DATASET LIMITS FOR TESTING\n",
    "    # ===========================================\n",
    "    X_train_cmds = X_train_cmds[:LIMIT]\n",
    "    y_train = y_train[:LIMIT]\n",
    "    \n",
    "    X_test_cmds = X_test_cmds[:LIMIT]\n",
    "    y_test = y_test[:LIMIT]\n",
    "\n",
    "    return X_train_cmds, y_train, X_test_cmds, y_test\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def get_tpr_at_fpr(predicted_probs, true_labels, fprNeeded=1e-4):\n",
    "    # if isinstance(predicted_logits, torch.Tensor):\n",
    "    #     predicted_probs = torch.sigmoid(predicted_logits).cpu().detach().numpy()\n",
    "    # else:\n",
    "    #     predicted_probs = sigmoid(predicted_logits)\n",
    "    \n",
    "    if isinstance(true_labels, torch.Tensor):\n",
    "        true_labels = true_labels.cpu().detach().numpy()\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(true_labels, predicted_probs)\n",
    "    if all(np.isnan(fpr)):\n",
    "        return np.nan#, np.nan\n",
    "    else:\n",
    "        tpr_at_fpr = tpr[fpr <= fprNeeded][-1]\n",
    "        #threshold_at_fpr = thresholds[fpr <= fprNeeded][-1]\n",
    "        return tpr_at_fpr#, threshold_at_fpr\n",
    "\n",
    "X_train_cmds, y_train, X_test_cmds, y_test = load_data()\n",
    "print('X_train_cmds: ', len(X_train_cmds))\n",
    "print('X_test_cmds: ', len(X_test_cmds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sigma(X_cmds, y, sigma_yml, verbose=False):\n",
    "    with open(sigma_yml, \"r\") as f:\n",
    "        sigma = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    patterns = sigma['detection']['keywords']\n",
    "    \n",
    "    y_pred = []\n",
    "    for i, cmd in enumerate(X_cmds):\n",
    "        # match if pattern in cmd\n",
    "        for pattern in patterns:\n",
    "            #if re.search(pattern, cmd):\n",
    "            if pattern in cmd:\n",
    "                if verbose:\n",
    "                    print(f\"[!] True label: {y[i]} | Found '{pattern}' in '{cmd}'\")\n",
    "                y_pred.append(1)\n",
    "                break\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "        \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = apply_sigma(X_test_cmds, y_test, sigma_yml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266501\n",
      "235069\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(y_test))\n",
    "print(len(y_test) - np.sum(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP 8974\n",
      "FP 0\n"
     ]
    }
   ],
   "source": [
    "mask = y_test == 1\n",
    "true_positives = np.array(y_test_pred)[mask]\n",
    "false_positives = np.array(y_test_pred)[~mask]\n",
    "print(\"TP\", np.sum(true_positives))\n",
    "print(\"FP\", np.sum(false_positives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR at FPR=1e-4: 0.0337\n",
      "Accuracy: 0.4866\n",
      "F1 score: 0.0652\n",
      "AUC: 0.5168\n"
     ]
    }
   ],
   "source": [
    "tpr = get_tpr_at_fpr(y_test_pred, y_test, fprNeeded=1e-4)\n",
    "acc = accuracy_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "auc = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "print('TPR at FPR=1e-4: {:.4f}'.format(tpr))\n",
    "print('Accuracy: {:.4f}'.format(acc))\n",
    "print('F1 score: {:.4f}'.format(f1))\n",
    "print('AUC: {:.4f}'.format(auc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
